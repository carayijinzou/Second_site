[{"authors":["admin"],"categories":null,"content":"Cara Zou is a rising senior and pre-dental student at UT Austin. She\u0026rsquo;s an ARI research mentor in the Virtual Drug Screening lab. Her hometown is Katy, Texas but she has also lived in China, Canada, Malalysia, and Singpaore.\nWhat is the purpose of this website? To showcase some of my projects and what I\u0026rsquo;ve learned so far. This website also acts as some of my notes for these topics and hopefully, if you are also a bioinformatics student, it can maybe help you too! But, disclaimer, while I try my best to explain topics, I\u0026rsquo;m still learning so there may be mistakes.\n","date":1554595200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1567641600,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/author/cara-yijin-zou/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/cara-yijin-zou/","section":"authors","summary":"Cara Zou is a rising senior and pre-dental student at UT Austin. She\u0026rsquo;s an ARI research mentor in the Virtual Drug Screening lab. Her hometown is Katy, Texas but she has also lived in China, Canada, Malalysia, and Singpaore.","tags":null,"title":"Cara (Yijin) Zou","type":"authors"},{"authors":["吳恩達"],"categories":null,"content":"吳恩達 is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n","date":1461110400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1555459200,"objectID":"da99cb196019cc5857b9b3e950397ca9","permalink":"/author/%E5%90%B3%E6%81%A9%E9%81%94/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/%E5%90%B3%E6%81%A9%E9%81%94/","section":"authors","summary":"吳恩達 is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.","tags":null,"title":"吳恩達","type":"authors"},{"authors":null,"categories":null,"content":"Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026quot;Courses\u0026quot; url = \u0026quot;courses/\u0026quot; weight = 50  Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026quot;Docs\u0026quot; url = \u0026quot;docs/\u0026quot; weight = 50  Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example1/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"/talk/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":null,"categories":["Bioinformatics","Computer Science"],"content":" This post is used to illustrate a simple version of implementing the greedy algorithm for sequence assembly in python. Please keep in mind that there are many more complex and robust ways.\nGreedy Algorithm Unlike other algorithms such as brute force of dynamic programming, the greedy algorithm may NOT always give the optimum solution. Instead, it looks at the best short term gain without consideration for the end result. One example of an algorithm that uses the Greedy algorithm is Kruskal’s algorithm (Minimum Spanning Tree), however, in this case, it does give the optimal solution.\n Sequencing One important tool used in biology is the sequencing of genetic data. There are various types of sequencing including sanger sequencing, illumina, nanopore, etc. Because techniques can be limited by read length, sometimes an original sequence/genome is fragmented, sequenced, and then assembled together again into a “contig” sequence (shot gun sequencing).\nFor example, given the following:\nGenome: ATTCGTAGCA\nCopy:\n ATTCGTAGCA ATTCGTAGCA ATTCGTAGCA  Fragmentation:\n  Frag1 Frag2 Frag3    ATT CGTA GCA  ATTC GTA GCA  AT TCGT AGCA     Limitations The shorter the reads, the harder it is to piece together (sometimes unsolvable). Another important factor to consider is if the original sequence has a lot of repeats such as ATTAATTAA which will cause it to fail. We also have to assume that the sequence read was perfect (no errors in the reading of the base) and that all reads are bridged (there is some overlap).\nAlso, get all unique sequences (duplicates don’t offer more information)\n Sequence Assembly Using the Greedy Algorithm Search for overlaps and where you can extend this. If there is more than 1 sequence that overlaps, find the sequence overlap with the highest area of overlap. Find two fragments with largest overlap and merge fragments. Repeat until only 1 fragment is left.  The following program was used to find the highest number of overlapping characters in two sequences, making sure that the match isn’t in the middle of the two sequences such as “ATGCGTA” and “GACGCGGGC”. Keep in mind that this program finds overlaps with the potential of left overhangs on the first sequence and right overhangs on the second sequence and NOT vice versa. For example: “AGCTTA” and “TTACCC”. (Probably not the most efficient way of doing it)\ndef find_high_overlap(string1_original, string2_original): string1 = string1_original string2= (len(string1_original)-1)* \u0026quot; \u0026quot; + string2_original highest_overlap=0 for k in range (len(string1_original)): overlap_num=0 #check individual characters to see if they match for i in range(len(string1)): overlap_num=0 if string1[i]==string2[i]: for w in range(i, len(string1)): if string1[w]==string2[w]: overlap_num+=1 else: overlap_num=0 break #if the match is in the middle of the sequences elif string1[i]!=\u0026quot; \u0026quot; and string2[i]!=\u0026quot; \u0026quot;: overlap_num=0 break if overlap_num \u0026gt; highest_overlap: highest_overlap = overlap_num if overlap_num \u0026gt; highest_overlap: highest_overlap = overlap_num string2= string2[1:]+\u0026quot; \u0026quot; return (highest_overlap) find_high_overlap(\u0026quot;ATGGCGAGC\u0026quot;,\u0026quot;GAGCATGGCGAGC\u0026quot;) ## 4 find_high_overlap(\u0026quot;GAGCATGGCGAGC\u0026quot;,\u0026quot;ATGGCGAGCCCAA\u0026quot;) ## 9 Okay, so now that we have a way of determining the repeats, now what? We need to build a n by n matrix where n is the number of fragments that we have. The value in the _i_th row and _j_th column is the overlap between the _i_th fragment and the _j_th fragment. Ignoring the diagonal from the top left to the bottom right, find the highest overlap (obviously a sequence will have the most overlap with itself). Once you find the two with the highest overlap (if more than two pairs, randomly pick a pair), merge/extend the pair into one fragment and change your matrix so that you have a n-1 by n-1 matrix. Continue doing this until there is only 1 fragment.\nThis is NOT the only way of solving this problem. As found in the references, you can also use a graph for example,where the node represents the fragment and the edge represents the number of highest overlap.\nBefore actually using the greedy algorithm to solve this problem, we must first build a program that merges two fragments together: This will assume that the overlap was found with the program above– no overlapping only in the middle of the two sequences, left overhangs on the first sequence, and right overhangs on the second sequence.\ndef merge(string1_original, string2_original, found_overlap): string1 = string1_original string2= (len(string1_original)-1)* \u0026quot; \u0026quot; + string2_original for k in range (len(string1_original)): overlap_num=0 #check individual characters to see if they match for i in range(len(string1)): overlap_num=0 if string1[i]==string2[i]: for w in range(i, len(string1)): if string1[w]==string2[w]: overlap_num+=1 else: overlap_num=0 break if overlap_num == found_overlap: #count the number of left spaces in the second sequence spaces=0 for j in range(len(string2)): if string2[j]==\u0026quot; \u0026quot;: spaces+=1 else: break merged_String= string1[:spaces]+string2[spaces:] return (merged_String.strip()) string2= string2[1:]+\u0026quot; \u0026quot; merge(\u0026quot;ATGGCGAGC\u0026quot;,\u0026quot;GAGCATGGCGAGC\u0026quot;, 4) ## \u0026#39;ATGGCGAGCATGGCGAGC\u0026#39; merge(\u0026quot;GAGCATGGCGAGC\u0026quot;,\u0026quot;ATGGCGAGCCCAA\u0026quot;, 9) ## \u0026#39;GAGCATGGCGAGCCCAA\u0026#39; Now let’s actually make the matrix:\ndef no_merges(matrix_1): for row in range(len(matrix_1)): for col in range(len(matrix_1)): if matrix_1[row][col]\u0026gt;0: return False return True def sequence_assembly(frag_list): #keep looping until only one fragment is left for frag in range(len(frag_list)-1): seq_matrix = [] for item in frag_list: seq_matrix.append(len(frag_list) * [0]) for row in range(len(seq_matrix)): for col in range(len(seq_matrix)): #the diagonal is how much it overlaps with itself (not useful) if row==col: seq_matrix[row][col]=-1 else: seq_matrix[row][col]=find_high_overlap(frag_list[row], frag_list[col]) print(\u0026quot;Matrix after\u0026quot;, frag, \u0026quot;merges\u0026quot;) prettyPrint(seq_matrix) if no_merges(seq_matrix)==True: print(\u0026quot;Cannot merge all\u0026quot;) return(frag_list) #find the highest overlap (if they overlap the same amount, this code just takes the first one it encounters) highest= [0] for i in range(len(seq_matrix)): for j in range(len(seq_matrix)): if seq_matrix[i][j]\u0026gt; highest[0]: highest[0]=seq_matrix[i][j] if len(highest)\u0026gt;1: highest[1]=i highest[2]=j else: highest.extend([i,j]) #print(highest) seq1=frag_list[highest[1]] seq2=frag_list[highest[2]] frag_list.append(merge(seq1,seq2, highest[0])) frag_list.remove(seq1) frag_list.remove(seq2) return (frag_list) frag_list=[\u0026quot;GAGCATGGCGAGC\u0026quot;,\u0026quot;ATGGCGAGCCCAA\u0026quot;,\u0026quot;ATGGCGAGC\u0026quot;,\u0026quot;CAATGCACCA\u0026quot;] sequence_assembly(frag_list) ## Matrix after 0 merges ## -1 9 9 1 ## 0 -1 1 3 ## 4 9 -1 1 ## 0 1 1 -1 ## ## Matrix after 1 merges ## -1 1 4 ## 1 -1 0 ## 1 3 -1 ## ## Matrix after 2 merges ## -1 1 ## 3 -1 ## ## [\u0026#39;ATGGCGAGCATGGCGAGCCCAATGCACCA\u0026#39;] frag_list_2=[\u0026quot;ATGGCGAGC\u0026quot;,\u0026quot;CAATGCACCA\u0026quot;, \u0026quot;GAGCATGGCGAGC\u0026quot;,\u0026quot;ATGGCGAGCCCAA\u0026quot;] sequence_assembly(frag_list_2) ## Matrix after 0 merges ## -1 1 4 9 ## 1 -1 0 1 ## 9 1 -1 9 ## 1 3 0 -1 ## ## Matrix after 1 merges ## -1 0 1 ## 1 -1 9 ## 3 0 -1 ## ## Matrix after 2 merges ## -1 0 ## 3 -1 ## ## [\u0026#39;GAGCATGGCGAGCCCAATGCACCA\u0026#39;] As demonstrated in the above example, the following fragments: “GAGCATGGCGAGC”,“ATGGCGAGCCCAA”,“ATGGCGAGC”,“CAATGCACCA” can be used to form the longer sequence. However, depending on the order of the sequences submitted, the resulting longer sequence can be different: ‘ATGGCGAGCATGGCGAGCCCAATGCACCA’, ‘GAGCATGGCGAGCCCAATGCACCA’, etc. Why? Because as stated previously, if there is more than 1 pair of sequences that overlap the highest number of character overlaps, my program takes the first one it encounters–therefore order matters in this case.\nThere may also be cases where the greedy algorithm cannot merge all of the fragments into one:\nsequence_assembly([\u0026quot;ACGGAAATAC\u0026quot;, \u0026quot;ATCAGGT\u0026quot;, \u0026quot;GGTAAAG\u0026quot;]) ## Matrix after 0 merges ## -1 0 0 ## 0 -1 3 ## 0 0 -1 ## ## Matrix after 1 merges ## -1 0 ## 0 -1 ## ## Cannot merge all ## [\u0026#39;ACGGAAATAC\u0026#39;, \u0026#39;ATCAGGTAAAG\u0026#39;]  References AN INTRODUCTION TO BIOINFORMATICS ALGORITHMS, NEIL C. JONES AND PAVEL A. PEVZNER http://data-science-sequencing.github.io/Win2018/lectures/lecture6/#:~:text=The%20greedy%20algorithm%20assembles%20the%20reads%20into%20an%20incorrect%20DNA,greedy%20approach%20can%20still%20fail.\u0026amp;text=One%20can%20think%20of%20the,looking%20for%20length%2D%E2%84%93%20overlaps. https://ocw.mit.edu/courses/biology/7-91j-foundations-of-computational-and-systems-biology-spring-2014/lecture-slides/MIT7_91JS14_Lecture6.pdf   ","date":1595030400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595030400,"objectID":"b49abd2028e3c78708b77614f2a79359","permalink":"/post/greedy_algorithm/","publishdate":"2020-07-18T00:00:00Z","relpermalink":"/post/greedy_algorithm/","section":"post","summary":"The greedy algorithm can be used to assemble longer sequences from shorter fragments from sequencing, not always giving the optimal result.","tags":["Bioinformatics","Computer Science","Greedy Algorithm","Sequence Assembly"],"title":"Greedy Algorithm and Sequence Assembly","type":"post"},{"authors":null,"categories":["Bioinformatics","VDS"],"content":" PyMOL Brief Introduction: What is PyMOL? PyMOL is a molecular visualization software used for 3D depictions of molecules such as proteins, nucleic acids, etc. You can download the software here: https://pymol.org/2/.\n Why do scripting and not just use the graphical interface? While the graphical interface can seem less daunting and has less of a learning curve, scripting allows you to go back and change certain details that may be difficult just using the graphical interface.\n How do I create a script? In PyMOL, go to file –\u0026gt; log file –\u0026gt; Open. Name your new script and save as a .pml. You can start typing commands in PyMOL and your actions will be recorded. To stop recording, go to log file –\u0026gt; Close. To run this script later on, go to run script and find the file. You always open the .pml in a text editor and change commands later.\n How to approach/ (Hopefully) Helpful tips Having a pymol “cheat sheet” can aslo be extremely helpful. Here are some key details that I find to be useful. These are just some functions that I think are important, for more information, here is the pymol wiki.\nSetting up the interface  bg white\n  hide nonbonded # hides water molecules\n  set seq_view, 1\n  Loading in a protein(s) Sometimes, it can appear that the script works fine when manually inputting lines one by one but when running an entire script, it fails to run. In this case, it is important to use the async=0 so that PyMOL understands to pause until the protein(s) has loaded before proceeding. Also something to note, currently, when fetching directly from PyMOL, files download as a .CIF instead of a .PDB. You can change this by adding type=pdb.\n fetch 3lwb, type=pdb, async=0\n  Simple (self-explanatory) functions  zoom resn EDO\n  show sticks, hyfo\n  show spheres, 3lwb_active_water\n  color orange, hyfo\n  orient 3lwb\n  No Ctrl+Z (Undo) Something I find really annoying about PyMOL is that undo doesn’t really work (this is also why I think pymol scripting is so great). Previously, I always kept saving sessions because I was afraid of losing progress (which you should still do, just in case). Something else that I recently found out about is using scenes. It acts like a snapshot in a moment in time. You can switch between different scenes that you saved and you can also use these scenes to create movies. Scenes are great because you can go back to a certain scene and work based on that if you messed up in the GUI. However, it is also important to note that some functions such as mutagenesis are still permanent in that it will change previous scenes as well.\n scene 001, store #save scene\n  save 3lwb_v1.pse #save session\n  Select (and Rename) Objects select chosen_name, resn actual_name\n sele protein, chain A - resn HOH\n  select ADP, (1iow \u0026amp; resn ADP)\n  select backbone, name CA+N+C+O\n  sele active_site, resi 122+120+189+208+250+124\n  select hyfo, resn ala+gly+val+ile+leu+phe+met\n  Labelling  label (resi 330 \u0026amp; n;cg \u0026amp;! 1iow),“%s-%s” % (resn,resi)\n  label (ADP \u0026amp; n;o3a), “ADP”\n  label (resn MG), “Magnesium”\n  Booleans Reviewing booleans for “and”, “or”, “not” can be helpful when selecting molecules. For example:\n select 3lwb_active_water, ((ligands) around 3.2) and (resn HOH) \u0026amp; ! 1iow\n  Alter and Rebuild When changing the size of spheres, it may be necessary to rebuild the view in order to see the change.\n alter Magnesium, vdw = 0.7\n  rebuild\n  Acronyms Other than just practicing, I think the biggest help for learning scripting is learning what the acronyms and certain key words mean.\n  Command What it means    util.cbag Utility color by atom (color first letter)  resn ALA Residue name  resi 100 Residue identifier  id Column number in PDB  byres By complete residue (also byobject, bychain, etc.)  hetatm hetero-atoms (not part of protein)     Within VS Around  sele active, byres all within 5 of ligands\n Within also selects the ligands as part of the active site. If you want to select active site residues not including the ligands you can use around:\n select active, byres (ligand around 5)\n  Take high quality picture  ray\n  png 3lwbRay.png\n  Distance  distance hbonds_ligands, ligands, 3lwb_active, 3.2, mode=2\n  hide labels, hbonds_ligands\n  color black, hbonds_ligands\n  Atom names When selecting individual atoms it can be helpful to understand this notation: “n;ca”. This refers to name carbon alpha (and so on for beta, gamma, delta, etc.).\n Superimposing The first will move to superimpose on the second structure.\n super 1iow, 3lwb\n    ","date":1593388800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593388800,"objectID":"802757d59ab26f0d290bf0d4dcf51645","permalink":"/post/pymol_scripting/","publishdate":"2020-06-29T00:00:00Z","relpermalink":"/post/pymol_scripting/","section":"post","summary":"PyMOL Brief Introduction: What is PyMOL? PyMOL is a molecular visualization software used for 3D depictions of molecules such as proteins, nucleic acids, etc. You can download the software here: https://pymol.","tags":["PyMOL","scripting","Visualization"],"title":"PyMOL Scripting","type":"post"},{"authors":null,"categories":["Dental","Bioinformatics"],"content":" Predicting the Effect of PAX9 variants using a Random Forest Model Abstract Pax9 is a gene belonging to the family of paired-box genes that encode transcription factors involved in organogenesis. Defects in Pax9 can lead to various types of cancer as well as a condition known as oligodontia which causes missing teeth in individuals. A random forest model was used to predict the phenotypic effect of variants of PAX9. Features that were used include: location of mutation, type of mutation, CADD score, protein change (BLOSUM80 score), and in-frame/out of frame mutation. A 10-fold cross validation was conducted to ensure that the model was not overfitting. The resulting random forest model had an 84.0% prediction accuracy, in which the sensitivity was 89.5%, the specificity was 90.9%, and the precision was 89.5%. However, due to the small size of the available data there are many limitations to the model and involving other features such as other gene interactions would be a major direction of future research on the PAX9 gene.\n Introduction/Background The PAX9 gene belongs to the family of paired box genes (PAX), which are highly conserved across species. It is located on chromosome 14 and codes for a transcription factor which plays a key role in fetal development. Mutations in the PAX9 gene can lead to different types of cancers (especially lung cancer) as well as oligodontia, a congenital defect of more than 6 missing teeth, especially the molars.The members of the PAX gene family usually consist of a paired-type homeodomain, an octapeptide and a paired box domain. The paired box domain is responsible for binding to DNA to facilitate transcription and is one of our major interests to examine whether a mutation in the gene encoding this structure can be an indication of potential cancer or tooth agenesis phenotype. There are five exons that make up the gene transcript for PAX9. The two most important exons for our project include exon 2 (37130902-37131298), which contains four nucleotides including the start codon, as well as exon 3 (37132102-37132728) which contains the majority of the paired box domain.\nThe features used by the random forest model includes: location of mutation, type of mutation, CADD score, protein change (BLOSUM80 score), and in-frame/out of frame mutation. The location of the mutation was important because preliminary analysis using a chi-squared test determined that the location of the mutation (in the paired box region or not) is not independent to whether an individual exhibits oligodontia. Additionally, many of the features were chosen based on known biological information of genes/proteins. Type of mutation and in frame/out of frame mutation were chosen because synonymous mutations are likely to have little deleterious effect while missense and nonsense mutations are likely to cause a more significant effect on the gene product. Insertions and deletions are likely to cause the most effect on the gene since this can cause a frameshift and thus affect all subsequent amino acids. The effect of the protein change was quantified using a higher valued BLOSUM (BLOSUM80) which is used for closely related sequences as some amino acid changes are more detrimental than others. Finally, CADD score was used as it has a similar objective to our model (predicting the effect of variants) and including other similar models will hopefully improve the ability of our model.\n Objective Utilizing a random forest to classify whether a PAX9 gene variant is pathogenic with the gene data from ClinVar, cBioPortal and gnomAD.\n Data Sources The pathogenic variants were found using cBioPortal (https://www.cbioportal.org/) which contained known variants that cause cancer. Pathogenic variants were also identified from the ClinVar database (https://www.ncbi.nlm.nih.gov/clinvar/) which consists of variants and their corresponding phenotype/condition. ClinVar largely contained variants that resulted in tooth agenesis. If the clinical significance was labelled as “benign” or “likely benign”, they were excluded from the positive variants dataset. It must be noted that there is a risk using ClinVar since there are more lenient standards for submitting data onto the database.\nThe negative variants were extracted from gnomAD (https://gnomad.broadinstitute.org/) which is an aggregation of genome sequencing data. Although variants from individuals that are affected by severe pediatric disease as well as their first degree relatives were removed, there may still be some individuals affected by severe disease still included. Therefore, this is not a true reference set of alleles. To remove some known disease variants, any overlap between gnomAD and the pathogenic variants were removed from the gnomAD dataset provided that the allele frequency was less than 0.05.\nAll variants were relative to the GRCh37 reference genome (NG_013357.1). ### Computational Methods Various R packages were used including ‘tidyverse’ and ‘ggplot2’ for manipulating the data as well as ‘peptider’ which contained the BLOSUM80 matrix. The ‘randomForest’ package provided the model for machine learning.\nThe Ensembl Variant Effect Predictor (VEP) was used to obtain the protein consequence for the ClinVar data while the other two datasets already included protein consequence. A VCF file was produced using the results of VEP and was used to obtain the PHRED score from Combined Annotation Dependent Depletion (https://cadd.gs.washington.edu/) which predicts the deleteriousness of a single nucleotide change as well as insertions and deletions.\n Results Variant Selection Since there are a lot more variants from gnomAD that are neutral phenotype compared to the number of pathogenic variants, an unbalanced data would form if all of the datasets were included. To combat this, we sampled an equal number of variants that are diseased/not diseased each time we trained our model. Additionally, we used the ten-fold cross validation to make sure that the data is not overfit.\n Features Allele frequency was originally a feature, however, ClinVar did not contain allele frequency and any attempts at obtaining allele frequency using the Ensembl allele frequency calculator were not successful because it only retrieves information for variants identified from the 1000 Genomes Project. While the cBioPortal dataset did have allele frequency, this was the frequency of a cancerous cell with the PAX9 variant out of all the biopsy samples which was not directly comparable to the gnomAD allele frequency value which is in reference to the entire population. Therefore, this feature was removed.\nGene Location The feature ‘gene location’ was derived from all the variant positions (only using the starting position for convenience) on the chromosome in reference to the human genome GRCh37. The classification of position was built up according to the UCSC genome browser with reference sequence NM_006194. Additionally, we enriched the classification result via the usage of the NCBI analysis on NM_006194.3 since USCS genome browser only provided the exon position. As a result, categories in this feature include:\n 5’ UTR (37126773-37,131,294) (covering all the exon 1 and majority of exon 2)\n Start codon region (37131295-37131298) (at the very end of exon 2)\n Exon 3 (37132102-37132728) (contain PAI, RED and KDM5B)\n PAI (37132116-37132286)\n RED (37132341-37132487)\n KDM5B (37132599-37132664)\n Exon 4 (37135667-37135806)\n Exon 5 (37145403-37145657)\n 3’ UTR (37,145,658-37147011)\n Intron (parts outside of the above regions)\n   Paired box region The PAI stands for paired-box region, which encodes a structure for the PAX9 transcription factor to bind to DNA molecules. The RED subdomain aids and corrects PAI’s recognition of DNA sites, and its variant number is larger than PAI, which might account for why PAX proteins have different target genes. The KDM5B protein helps transcriptional repression of some tumor suppressor gene.\n BLOSUM80 (amino acid conservation) The BLOSUM80 score displayed the extent of effect of amino acid change on the stability of the PAX9 protein. We expected that this feature would greatly improve the prediction of our model. R package ‘peptider’ was used to obtain the BLOSUM80 matrix. However, the substitution matrix only considered single amino acid change, so we assumed that variants with a replacement length over one (such as a deletion, frameshift, etc) would get the lowest score since they are more likely to have deleterious effects on the protein structure.\n Mutation Type For the feature of mutation type, it was determined from the original data for gnomAD and cBioPortal. The ClinVar data was processed via VEP to acquire the result. Using the mutation feature and BLOSUM80 matrix, we were able to further classify the data into whether it was in-frameshift and out-of-frame shift since it was expected that frameshift mutation would likely to generate abnormal protein.\n CADD PHRED score CADD PHRED score was obtained by using the coordinates of variants and submitting to VEP. The result produced a VCF file that was submitted to CADD. Some variants such as fusion proteins did not have a CADD score and were given an estimated CADD score ( in this particular case, maximum CADD score was used in replacement) which may not have reflected its true deleteriousness effect.\n Pathogenicity Finally, for the pathogenicity feature–which was our goal of RF prediction–due to the missing allele frequency in most of the cBioPortal and ClinVar it would be nearly impossible to precisely know which variant was neutral or not. Thus, we had to assume that all ClinVar and cBioPortal variants were pathogenic while all gnomAD variants were neutral (after filtering for duplicates) for convenience.\n  Random Forest ## ## Call: ## randomForest(formula = phenotype ~ ., data = training_data, importance = T, ntree = 20, na.action = na.exclude) ## Type of random forest: classification ## Number of trees: 20 ## No. of variables tried at each split: 2 ## ## OOB estimate of error rate: 14.44% ## Confusion matrix: ## neutral pathogenic class.error ## neutral 75 15 0.1666667 ## pathogenic 11 79 0.1222222 The RF model showed that it had an overall accuracy of 85.56%, with 20 decision trees for doing classification tasks. The low value of tree number was due to the limited training data size (80).\n## neutral pathogenic MeanDecreaseAccuracy MeanDecreaseGini ## GRCh37Location 6.496823 9.856697 9.376161 25.2817657 ## gene_location 1.924800 3.587758 4.798873 8.1996621 ## BLOSUM 4.423561 7.614464 6.227631 17.4672859 ## Mutation_Type 2.271270 2.648797 2.938764 13.3248609 ## inframe 1.025978 0.000000 1.025978 0.4994845 ## PHRED 3.754689 1.829102 3.861181 19.7613239 Based on the variable importance plot above, the most important variable would be the location relative to the reference GRCh37. On the other hand, the variable with the smallest effect on the predicting power of our model is inframe. This is expected since there were relatively few variants that caused a frame shift in the population.\n Ten-fold Cross Validation ## 1 2 3 4 5 6 ## ten_fold_accuracy 0.8888889 0.7777778 0.8888889 0.8888889 0.7777778 0.8888889 ## 7 8 9 10 ## ten_fold_accuracy 0.8333333 0.9444444 0.8888889 0.8888889 Cross Validation mean accuracy:\n## [1] 0.8666667 By comparing the accuracy between RF (85.56%) and ten-fold (86.67%), we found that despite the low sample size, the model exhibited limited overfitting.\n Prediction Result ## prediction_result ## truth neutral pathogenic Sum ## pathogenic 5 17 22 ## neutral 20 2 22 ## Sum 25 19 44 ## accuracy sensitivity specificity precision ## pathogenic 0.8409091 0.8947368 0.9090909 0.8947368 From the confusion matrix, the model has relatively good accuracy, sensitivity, specificity, and precision.\n  Conclusion The objective of the project was to build a random forest model to predict the phenotype of a PAX9 gene variant. The ten-fold cross validation indicated that the model didn’t overfit. While the prediction result was satisfactory, it is important to note that it is only as good as the available training data. It is again acknowledged that there was a risk associated with using the ClinVar resource due to its relatively loose quality check. Additionally, since a large proportion of the allele frequency in cBioPortal and ClinVar was missing, it was difficult to identify all the potential neutral variants and all the variants were assumed to be pathogenic in order to run the RF model. Hence, if possible, allele frequency could potentially be an important feature of any future research on PAX9 gene. Another improvement would be to increase the very limited online human PAX9 gene data to generate more powerful machine learning models. To further refine the prediction of the RF model, a suggestion may be to use the continuous conservation score from other available tools as highly conserved sequences are likely less tolerant to changes. Although BLOSUM80 was insightful on the effect of amino acid substitution on protein structure, adding conservation score could provide a more comprehensive information. Lastly, future research may have a stronger focus on the PAX9’s metabolic pathway. This gene plays a principal role in organogenesis and interacts with many other genes so it is unfortunate that this gene hasn’t received the same attention as other popular genes like p53.\n References Bonczek et al, ‘PAX9 gene mutations and tooth agenesis: A review’, Clinical Genetics, 2017/02/01, doi: 10.1111/cge.12986. https://www.researchgate.net/publication/313264549_PAX9_gene_mutations_and_tooth_agenesis_A_review UCSC genome browser on PAX9. http://www.genome.ucsc.edu/cgi-bin/hgc?hgsid=828585015_qsdofxT4g0z4a48R33zLnyqdtxND\u0026amp;g=htcCdnaAliInWindow\u0026amp;i=NM_006194\u0026amp;c=chr14\u0026amp;l=37126772\u0026amp;r=37147011\u0026amp;o=37126772\u0026amp;aliTable=refSeqAli\u0026amp;table=refGene NCBI data on PAX9: https://www.ncbi.nlm.nih.gov/nuccore/NM_006194#feature_NM_006194.3 Pellizzari et al. Co-operation between the PAI and RED subdomains of Pax-8 in the interaction with the thyroglobulin promoter. Biochem J. 1999;337 ( Pt 2)(Pt 2):253‐262. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1219959/ KDB5B gene: https://www.genecards.org/cgi-bin/carddisp.pl?gene=KDM5B Ensembl Variant Effect Predictor (VEP): https://useast.ensembl.org/info/docs/tools/vep/index.html Combined Annotation Dependent Depletion (CADD): https://cadd.gs.washington.edu/ cBioPortal: https://www.cbioportal.org/ ClinVar:https://www.ncbi.nlm.nih.gov/clinvar/ gnomAD: https://gnomad.broadinstitute.org/ Random Forest: A. Liaw and M. Wiener (2002). Classification and Regression by randomForest. R News 2(3), 18–22. Tidyr package: Hadley Wickham and Lionel Henry (2020). tidyr: Tidy Messy Data. R package; version 1.0.2. https://CRAN.R-project.org/package=tidyr Peptider package: Heike Hofmann, Eric Hare and GGobi Foundation (2015). peptider: Evaluation of Diversity in Nucleotide Libraries. R package version 0.2.2. https://CRAN.R-project.org/package=peptider Dplyr package: Hadley Wickham, Romain François, Lionel Henry and Kirill Müller (2020). Dplyr: A Grammar of Data Manipulation. R package version 0.8.5. https://CRAN.R-project.org/package=dplyr    ","date":1588896000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588896000,"objectID":"a9e9ce92408f2a8fe9813343eabd7080","permalink":"/project/pax9_rf/","publishdate":"2020-05-08T00:00:00Z","relpermalink":"/project/pax9_rf/","section":"project","summary":"Pax9 is a gene belonging to the family of paired-box genes that encode transcription factors involved in organogenesis. Defects in Pax9 can lead to various types of cancer as well as a condition known as oligodontia which causes missing teeth in individuals. A random forest model was used to predict the phenotypic effect of variants of PAX9.","tags":["Model","PAX9","Dental","Oligodontia","Random Forest","Cross Validation","Bioinformatics","Machine Learning","Supervised"],"title":"PAX9 Variants Random Forest Model","type":"project"},{"authors":null,"categories":["Statistics","Probability","Bioinformatics"],"content":" Hidden Markov Model I go over the basics of a Markov Chain here, however, unlike chutes and ladders, for a hidden markov model, there is hidden information. In this case the markov chain is not visible to us, we can only observe outcome values.\nLike a markov chain, there are still states (now hidden), transition probabilities, and initial probabilities. However, in addition to this, we also have an observed state and an emission probability.\n Netflix    Netflix   Sleep    Sleep   Homework    Homework   0.4    0.4   0.4    0.4   0.5    0.5   0.1    0.1   0.4    0.4   0.6    0.6   0.1    0.1   0.2    0.2   0.3    0.3   Stressed    Stressed   Relaxed    Relaxed   Stressed    Stressed   Relaxed    Relaxed   Stressed    Stressed   Relaxed    Relaxed   0.1    0.1   0.9    0.9   0.3    0.3   0.7    0.7   0.9    0.9   0.1    0.1    Viewer does not support full SVG 1.1    Emission probability The emission probability is what gives way to the observed state. In other words, given the state \\(\\pi\\), what is the probability of the observed state \\(x\\) happening at time \\(i\\).\n\\[ P (x_{i}|\\pi_{i}) = \\textrm{emission probability} \\]\nStill using the example from before, the difference now is that you don’t know if the student is sleeping, watching netflix, or doing homework. Instead, the only information you have is whether or not the student is stressed (observed state). Depending on your actual state (sleep, homework, netflix), the probability of being stressed changes.\n   Relaxed Stressed    Sleep 0.9 0.1  Homework 0.1 0.9  Netflix 0.7 0.3    For reference, here is the initial probabilities from before:\n  Outcome Likelihood    Sleep 0.6  Homework 0.3  Netflix 0.1    As well as the transition probabilities: Keep in mind: \\(P (\\pi_{i-1} \\rightarrow \\pi_{i})\\) and \\(P(\\pi_{i}|\\pi_{i-1})\\)\n   Sleep Homework Netflix    Sleep 0.5 0.4 0.1  Homework 0.4 0.2 0.4  Netflix 0.3 0.1 0.6    Example    Observations \\(x\\) (1= stressed, 0=relaxed) 1 1 0 0 1 1 1 0 0    State Path \\(\\pi\\) (S= sleep, H=homework, N=netflix) H H N N N H S S S  Emission Probability \\(P (x_{i}|\\pi_{i})\\) 0.9 0.9 0.7 0.7 0.3 0.9 0.1 0.9 0.9  Transition Probability \\(P (\\pi_{i-1} \\rightarrow \\pi_{i})\\) 0.3 0.2 0.4 0.6 0.6 0.1 0.4 0.5 0.5     What is the likelihood of a specific observation sequence? \\[ P (x)= \\sum_{\\pi} P(x \\textrm{ and }\\pi) \\]\nWhat the above equation describes is that to determine the likelihood of an observation sequence, you must add the probabilities of the observed sequence and the potential (hidden) state paths occurring. So how do we do this???\nBased on conditional probability:\n\\[ P(x \\textrm{ and } \\pi) = P (x | \\pi) \\cdot P(\\pi) \\]\nMultiply the emission probabilities together: Given a state sequence, what is the probability of emitting the observed sequence. \\[ P (x | \\pi) = \\prod_{t=1} P (x_{t}|\\pi_{t}) \\] Multiply the transition probabilities together: Probabililty of a state sequence depends on the markov property that the state at time t is only dependent on the state at t-1. \n\\[ P(\\pi) = \\prod_{t=1} P (\\pi_{t}|\\pi_{t-1}) \\]\nPutting it all together: The probability of an observed sequence and a corresponding hidden state sequence is equal to the product of the emission probabilities multiplied by the product of the transition probabilities.\n\\[ P(\\pi \\textrm{ and } x) = \\prod_{t=1} P (\\pi_{t}|x_{t}) \\cdot \\prod_{t=1} P (\\pi_{t}|\\pi_{t-1}) \\] As an example:\nWe could potentially state all possible state paths given a particular observations. If we observed: {1, 0, 0}, the possible state paths include SSS, SHH, SHN, SNN, SNH, HHH, HNS, HSH, HSS, etc. We could calculate the possibility of each, for example: SNS. Initial probability of sleep is 0.6. The probability of being stressed given that they are sleeping is 0.1. Sleep transitioning to Netflix is 0.1.Probability of being relaxed watching Netflix is 0.3. And so on…\n\\[ P (\\pi_{1}=S | \\textrm{start}) \\cdot P(x_{1}=1|\\pi_{1}=S)\\cdot P ( \\pi_{2}=N | \\pi_{1}=S )\\cdot P(x_{1}=0|\\pi_{1}=N)\\cdot P (\\pi_{3}=S|\\pi_{2}=N)\\cdot P(x_{3}=0|\\pi_{3}=S) \\] \\[ P (\\pi=\\textrm{S N S} \\cap x=\\textrm{1 0 0})=0.6 \\cdot0.1\\cdot0.1\\cdot0.3\\cdot0.3\\cdot0.9 = 0.000486 \\]\nTo find the total probability of an observed sequence (e.g. 1,0,0), we need to add up all of the possible state path probabilities (SNS, SSS, HSN, etc.).\n\\[ P(\\textrm{1 0 0}) = P(\\textrm{1 0 0}| \\textrm{S N S})+P(\\textrm{1 0 0}| \\textrm{N N S}) +P(\\textrm{1 0 0}| \\textrm{S N H}) + ... \\] However, what if the observation sequence is long and therefore the corresponding hidden state sequence is also long which many possible permutations. Calculating the likelihood of a observation would take forever… which is why we use the forward algorithm.\n Forward algorithm The forward algorithm uses dynamic programming. What this means is that the problem is broken down into sub-problems which you find the optimal solution to in order to solve the larger problem. The solutions from the sub-problem are stored (memoization) so that you don’t waste time computing it over and over again.\nTo implement this, you need to create an empty matrix to store intermediate values. The rows equal the number of states (in this case, 3), while the columns should be equal to the number of values in the observation sequence.\n First column: The first column states the probability of each state being the initial state and emitting the first observation. All future calculations will be based on these values.\n Subsequent columns: Each value in the matrix represents the likelihood of being in a certain state k having observed the first i values in the observation sequence. We will represent this by:\n  \\[ P (\\pi_t=k \\textrm{ and } x = x_1, x_2 , x_3 , ...x_t) = \\alpha_t (k) \\] This is calculated by taking the emission probability of emitting \\(x_t\\) given that we are at state k and multiplying this by the summation of the probabilities of being at the previous state \\(l\\) at t-1 and transitioning to state k.\n\\[ \\alpha_t (k) = \\textrm{(emission probability of } x_{t} \\textrm{ given current state k)}\\sum \\alpha_{t-1} (l) \\cdot \\textrm{(transition probability from l to k)} \\] \\[ \\alpha_t (k) = P (x_{t}|\\pi_{t})\\sum \\alpha_{t-1} (l) \\cdot P (\\pi_{t}|\\pi_{t-1}) \\] OK, that’s just a ton of math, what does it actually mean? Still using the example from before with the observed sequence 1 0 0, if we have already calculated all the paths from 1 0, we then need to calculate the path to the next 0 based on these paths formed from 1 0. Therefore for each value, you must look at the values in the previous columns.\ndef forward(): # if Stressed = 0, Relaxed = 1 observations = [1, 0, 0, 1, 1, 0, 1, 1] #build empty dynamic programming matrix num_states= 3 obs_seq_num = len (observations) grid = [] for j in range(num_states): grid.append(obs_seq_num * [0]) # initial probability distribution for S, H, N initial = [0.6,0.3,0.1] # emission probability emission= [[0.9,0.1],[0.1,0.9],[0.7,0.3]] #transition probabilities transition = [[0.5,0.4,0.1],[0.4,0.2,0.4],[0.3,0.1,0.6]] #Start filling out the grid for t in range(obs_seq_num): for i in range(num_states): #first column if t == 0: # use the initial probability distribution grid[i][t] = initial[i] * emission [i][observations[t]] else: grid[i][t] = emission [i][observations[t]] * ((grid[0][t-1]*transition[0][i]) + (grid[1][t-1]*transition[1][i]) + (grid[2][t-1]*transition[2][i])) prettyPrint (grid) print (\u0026quot;Probability of observing: \u0026quot;) for k in observations: print (observations[k], end=\u0026quot; \u0026quot;) print () for s in range(obs_seq_num): sum=0 for q in range (num_states): sum+=grid[q][s] print (format(sum, \u0026quot;5.4f\u0026quot;), end = \u0026quot; \u0026quot;) forward() ## 1 2 3 4 5 6 7 9 ## S 0.0600 0.1323 0.0874 0.0061 0.0022 0.0064 0.0005 0.0002 ## H 0.2700 0.0081 0.0064 0.0371 0.0100 0.0004 0.0029 0.0008 ## N 0.0300 0.0924 0.0503 0.0124 0.0069 0.0058 0.0013 0.0006 ## ## Probability of observing: ## 0 1 1 0 0 1 0 0 ## 0.3600 0.2328 0.1441 0.0557 0.0191 0.0126 0.0047 0.0016 For example, how did we calculate the probability of being at state S at time 2 with the observations thus far being: 1 0.\n Forward Matrix    Forward Matrix   time = 1    time = 1   0.06    0.06   2    2   0.1323    0.1323   3    3   0.0874    0.0874   0.27    0.27   0.0081    0.0081   0.0064    0.0064   0.03    0.03   0.0924    0.0924   0.0503    0.0503   S    S   H    H   N    N    Viewer does not support full SVG 1.1   \\[ P(\\pi_{2}=S \\textrm{ and } x_2 = 0 )= 0.9\\cdot (0.06\\cdot 0.5 + 0.27\\cdot 0.4+0.03\\cdot0.3)=0.1323 \\] Finally, what is the probability of the following sequence: 1, 0, 0? This is equal to the sum of all the probabilities at the end time: \\[ P(x= \\textrm{1 0 0})= 0.0874 + 0.0064 + 0.0503= 0.1441 \\] Additional note: How do you find the probability of, for example, being at state S at time 3? This is just the likelihood probability from the matrix divided by the sum of the entire column (what we calculated above): \\[ \\frac{0.0874} {0.0874 + 0.0064 + 0.0503}=0.6065 \\]\n Decoding: Viterbi Algorithm The viterbi algorithm attempts to answer: Given a sequence of observations, what is the most likely corresponding state path? It also uses dynamic programming by going from the first observation to the current observation and finding the most likely path, a.k.a the max. To find the path, we must also use backtracking.\n\\[ P (\\pi = \\pi_1,\\pi_2, ...k \\textrm{ and } x = x_1, x_2 , x_3 , ...x_t) = v_t (k) \\] As you can see from the formula, it is very similar to forward except we use the max instead of the sum of previous probabilities.\n\\[ v_t (k) = \\max (v_{t-1}(l)\\cdot\\textrm{emission probability of }x_t \\textrm{ given state } k\\cdot\\textrm{transition probability from l to k}) \\] \\[ v_t (k) = \\max (v_{t-1}(l)\\cdot P (x_{t}|\\pi_{t})\\cdot P(\\pi_{t}|\\pi_{t-1})) \\]\nAgain, similar to the forward algorithm, you will need a matrix to store intermediate maximum values. Each value represents the probability of being at state \\(k\\) at time \\(t\\) given the observations until time \\(t\\) and having gone through the most probable path at time \\(t-1\\). To compute this, you recursively find the most probable paths to each cell.\n x = argmax    x = argmax   y=max    y=max    Viewer does not support full SVG 1.1   But what about the actual path? This is when we use backtracking and argmax instead of max. If you have a typical hyperbola, at the highest point, the y-value you obtain is the max, while the x-value in which you used as input into the function to obtain the highest point is the argmax.\n\\[ \\pi_t (k) = \\textrm{argmax} (v_{t-1}(l)\\cdot P (x_{t}|\\pi_{t})\\cdot P(\\pi_{t}|\\pi_{t-1})) \\]\ndef viterbi(): # if Stressed = 0, Relaxed = 1 observations = [1, 0, 0, 1, 1, 0, 1, 1] #build empty dynamic programming matrix num_states= 3 obs_seq_num = len (observations) grid = [] for j in range(num_states): grid.append(obs_seq_num * [0]) # initial probability distribution for S, H, N initial = [0.6,0.3,0.1] # emission probability emission= [[0.9,0.1],[0.1,0.9],[0.7,0.3]] #transition probabilities transition = [[0.5,0.4,0.1],[0.4,0.2,0.4],[0.3,0.1,0.6]] for t in range(obs_seq_num): for i in range(num_states): #first column if t == 0: # use the initial probability distribution grid[i][t] = initial[i] * emission [i][observations[t]] else: grid[i][t] = emission [i][observations[t]] * max ((grid[0][t-1]*transition[0][i]), (grid[1][t-1]*transition[1][i]), (grid[2][t-1]*transition[2][i])) prettyPrint (grid) print(\u0026quot;Probability of most probable path of 1, 0, 0, 1, 1, 0, 1, 1:\u0026quot;, grid[1][7]) path=\u0026quot;\u0026quot; for p in range(obs_seq_num): col= [grid [0][p], grid [1][p], grid [2][p]] index = col.index(max(col)) state_dict={0:\u0026quot;S\u0026quot;, 1:\u0026quot;H\u0026quot;, 2:\u0026quot;N\u0026quot;} path+=state_dict[index] print (\u0026quot;Most likely state path: \u0026quot; + path) viterbi() ## 1 2 3 4 5 6 7 9 ## S 0.0600 0.0972 0.0437 0.0022 0.0006 0.0010 0.0001 0.0000 ## H 0.2700 0.0054 0.0039 0.0157 0.0028 0.0001 0.0004 0.0001 ## N 0.0300 0.0756 0.0318 0.0057 0.0019 0.0008 0.0001 0.0000 ## ## Probability of most probable path of 1, 0, 0, 1, 1, 0, 1, 1: 6.611976345600003e-05 ## Most likely state path: HSSHHSHH  Viterbi Matrix    Viterbi Matrix   time = 1    time = 1   0.06    0.06   2    2   0.0972    0.0972   3    3   0.0473    0.0473   0.27    0.27   0.0054    0.0054   0.0039    0.0039   0.03    0.03   0.0756    0.0756   0.0318    0.0318   S    S   H    H   N    N    Viewer does not support full SVG 1.1   For example, how did they calculate the probability of the most probable path at time 2 if the last state is S? The maximum of the time \\(t-1\\) is if the sequence started with H.\n\\[ P (\\pi = \\pi_1, ...S \\textrm{ and } x = x_1, ...0)= 0.27 \\cdot 0.4 \\cdot 0.9=0.0972 \\] The highest probability in the very last column of the 1st matrix gives us the probability of the most probable path. Therefore, the probability of the most probable path of 1, 0, 0, 1, 1, 0, 1, 1: 6.611976345600003e-05.\nTo find the path, go to each column and find the state with the maximum probability at the time point, for example, in this case, it is: HSSHHSHH.\n Packages This is too much math/I don’t care. There are plenty of packages available on the internet, for example, these methods are implemented in the R package, HMM, which I think is super helpful. Just understanding what the different terms mean such as transition matrix, initial probability distribution, emission probability, etc. should be enough to use these tools.\n Application This is a good exercise to test your understanding. It looks at predicting the secondary structure of proteins from the primary sequence. Another common application in bioinformatics is detecting CpG islands.\n References https://www.cs.rochester.edu/u/james/CSC248/Lec11.pdf http://pages.cs.wisc.edu/~matthewb/pages/notes/pdf/hmms/ForwardAlgorithm.pdf https://web.stanford.edu/~jurafsky/slp3/A.pdf https://www.cis.upenn.edu/~cis262/notes/Example-Viterbi-DNA.pdf    ","date":1587340800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587340800,"objectID":"aa7b9890ec7af676928cf01ab66ebde2","permalink":"/post/hidden_markov_model/","publishdate":"2020-04-20T00:00:00Z","relpermalink":"/post/hidden_markov_model/","section":"post","summary":"Hidden Markov Model I go over the basics of a Markov Chain here, however, unlike chutes and ladders, for a hidden markov model, there is hidden information. In this case the markov chain is not visible to us, we can only observe outcome values.","tags":["Statistics","Modelling","Markov Chain","Probability","HMM","Forward Algorithm","Viterbi Algorithm","Emission probability","Observed State","R"],"title":"Hidden Markov Model","type":"post"},{"authors":null,"categories":["Statistics","Probability"],"content":" Markov Chain What is a markov chain?  A succession of random events.\n There is a set of states, in the above example, after coming home from school: Sleep, Homework, and Netflix.\n There is an initial probability for each state. Right after coming home, you might be tired and there is a 60% probability of you taking a nap. You might have an assignment that is due that night so you might get started right away (30%) or you might have finished everything already and just want to relax and watch Netflix (10%).\n    Outcome Likelihood    Sleep 0.6  Homework 0.3  Netflix 0.1     There is a transition probability between states. For example: (For each probability, it is the likelihood of transitioning from the row to the column state.)     Sleep Homework Netflix    Sleep 0.5 0.4 0.1  Homework 0.4 0.2 0.4  Netflix 0.3 0.1 0.6     Application: Chutes and Ladders Markov Propoerty Markov property dictates that the future only depends on current state, thus it doesn’t matter how we got to current state. In other words, the probability of landing on a position is independent from all the moves before the current state.\nMarkov chains use a combination of probability and matrices for problems that occur in a series of steps or probability trees. In this way, we are able to calculate the probability of being in any state many steps ahead. Markov chains are a set of states and the probability of transitioning between states. In chutes and ladders, every player is in a state and no person can be in both states. Players change states when the dice is rolled.\n Transition Matrix These probabilities then form a matrix known as a transition matrix. The probability in the transition probability matrix is the probability of moving to a state, not the probability of starting in that state. All transition matrices are square which is due to the fact that each state always has a probability of transitioning to another state, even if the transition probability is zero.\n Absorbing Markov Chain There are different types of markov chains, however for the case of chutes and ladders, we will be focusing on one, the absorbing markov chain. An absorbing markov chain has absorbing states where once reached, it is impossible to exit from. In this case, we will be using an absorbing markov chain since the game is over once the final position is reached.\n Setting Up the Board In a typical board for chutes and ladders, there are 100 squares and thus many states that a player can be in, however this is tedius to construct, thus a simplified board is used to demonstrate this instead. In this case, the “ladder” is from position 2 and advances the player to position 8 while the “snakes” are from position 4 to position 1 and from position 9 to position 6. A standard, fair, six sided dice is used.\nPlayers are independent from each other, the moves of one player do not affect the probability of the other player on rolling a number and advancing. The only way the other person is involved is in who makes it to the 100th/last position first. Therefore, only one player’s moves need to be calculated and understood to understand this problem.\nIn this case, the state space is: {0,1,3,5,6,7,8,10}. This is because when landed on, for spaces 2,4,and 9, the position either moves up or down through the use of a ladder or snake. There are different types of states, position 0 in this case is an open state, once this state is exited, the player cannot go back. For position 10 in this case, it is an absorbing state as once it is reached the game is over. On the other hand, state 1,3,4,5,6,7,8 are transient states.\nInitial probability: All players start off the board (or at state “0”).  \\[P(X=0)=1\\]\nConstructing the transition matrix: Probability from left side column state to land on the top row state. (Why is it not 11 by 11? Keep in mind that only 7 states are possible). The transition probability matrix for the first roll is:  import numpy as np transition= np.array( [ [ 0, 1/3, 1/6, 1/6, 1/6, 0 , 1/6, 0 ], [ 0, 1/6, 1/6, 1/6, 1/6, 1/6, 1/6, 0 ], [ 0, 1/6, 0 , 1/6, 1/3, 1/6, 1/6, 0 ], [ 0, 0 , 0 , 0 , 1/3, 1/6, 1/6, 1/3], [ 0, 0 , 0 , 0 , 1/6, 1/6, 1/6, 1/2], [ 0, 0 , 0 , 0 , 1/6, 0 , 1/6, 2/3], [ 0, 0 , 0 , 0 , 1/6, 0 , 0 , 5/6], [ 0, 0 , 0 , 0 , 0 , 0 , 0 , 1 ]]) prettyPrint (transition) ## 0 1 3 5 6 7 8 10 ## 0 0.000 0.333 0.167 0.167 0.167 0.000 0.167 0.000 ## 1 0.000 0.167 0.167 0.167 0.167 0.167 0.167 0.000 ## 3 0.000 0.167 0.000 0.167 0.333 0.167 0.167 0.000 ## 5 0.000 0.000 0.000 0.000 0.333 0.167 0.167 0.333 ## 6 0.000 0.000 0.000 0.000 0.167 0.167 0.167 0.500 ## 7 0.000 0.000 0.000 0.000 0.167 0.000 0.167 0.667 ## 8 0.000 0.000 0.000 0.000 0.167 0.000 0.000 0.833 ## 10 0.000 0.000 0.000 0.000 0.000 0.000 0.000 1.000 Notice that the rows of the transition matrix add up to one. The expected value for the position of the player is calculated by the sum of, all the probability times their corresponding state number for the first horizontal row . Therefore, the expected value is 4.\npossible_states_int=[0,1,3,5,6,7,8,10] expected_one= np.dot(transition[0], possible_states_int) print(expected_one) ## 4.0 Matrix Multiplication After two rolls, the matrix is:  second= np.dot(transition, transition) prettyPrint(second) ## 0 1 3 5 6 7 8 10 ## 0 0.000 0.083 0.056 0.083 0.222 0.139 0.139 0.278 ## 1 0.000 0.056 0.028 0.056 0.222 0.111 0.139 0.389 ## 3 0.000 0.028 0.028 0.028 0.194 0.111 0.139 0.472 ## 5 0.000 0.000 0.000 0.000 0.111 0.056 0.083 0.750 ## 6 0.000 0.000 0.000 0.000 0.083 0.028 0.056 0.833 ## 7 0.000 0.000 0.000 0.000 0.056 0.028 0.028 0.889 ## 8 0.000 0.000 0.000 0.000 0.028 0.028 0.028 0.917 ## 10 0.000 0.000 0.000 0.000 0.000 0.000 0.000 1.000 expected_two= np.dot(second[0], possible_states_int) print(round(expected_two,2)) ## 6.86 This is found by multiplying the transition probability matrix by itself using dot product. The expected value after two rows is found by multiplying the matrix by itself. The expected value after two turns is 6.86.\nYou can continue this matrix multiplication determine the probability of being at a certain state after n number of rolls.\nExpected Number of rolls to finish  For expected hitting times or expected number of dice rolls to reach state 10, it can be found through the first-hitting-time model or the first passage time which indicates the time taken to reach a certain value.\n\\[ E(Y) = \\sum_{i}P(Y|X=i)P(X=i) \\]\nHow do we do this faster? Matrix partition\n\\[ \\begin{bmatrix} Q \u0026amp; R \\\\ 0 \u0026amp; I \\end{bmatrix} \\]\nTake the transition matrix and divide it into 4 (unequal) sections. In this case there are 6 transient states (0,1,3,5,6,7,8) and 1 absorbing state. Q, a square matrix, shows the transition probabilities between transient states (6 by 6 matrix). R is rectangular and represent the probability of a transient state turning into an absorbing state (6 by 1 matrix). 0 represents the reverse, the probability of an absorbing state turning reaching to a transition state (impossible based on the definition of absorbing state). I represents the identity matrix, once in this state, it can only stay in this state (absorbing state).\nSo how do we find the expected absorbing time? \\[ H = QH + 1 \\] The 1 represents the probability changing from absorbing to absorbing. Another way of writing this is: \\[ H = (I - Q)^{-1}1 \\]\n#since we only have one absorbing state, our Q square matrix is just the transition matrix without the last row and last column Q = transition[:-1,:-1] R = np.array([1,1,1,1,1,1,1]) I = np.identity(7) subtract = I-Q inverse = np.linalg.inv(subtract) np.dot(inverse,R) ## array([3.37232432, 3.10613256, 2.91389635, 2.05389222, 1.76047904, ## 1.50898204, 1.29341317]) Therefore, for the simplified chutes and ladders game, the expected number of moves from start to finish is 3.37 moves.\n Simulation Using Python, we can run a simulation to approximate this answer. Without having to write the transition matrix, it also fairly easy to adapt this to find the average number of turns for the full board and beyond.\ndef play_game(): chute_ladders = {4:1,2:8,9:6} win=False num_turns=0 position=0 while win!=True: num_turns+=1 #player spins dice=np.random.randint(1, high=7, dtype=int) #player moves position+= dice #check position if position\u0026gt;=10: win=True else: if position in chute_ladders: position = chute_ladders.get(position) return num_turns def main(): #run the simulation a bunch of times multipleGames = np.array([play_game() for i in range(10000)]) print(\u0026quot;Average number of moves to finish for 1000 games: \u0026quot;, np.mean(multipleGames)) main() ## Average number of moves to finish for 1000 games: 3.3584 We can see above that using a simulation, we get a similar answer to our markov chain of about 3.4 turns after averaging over 10,000 games (law of large numbers).\n Other Applications Markov chains are applicable to a wide range of topics including finance, music, and computer science. One example from biochemistry is Michaelis-Menten kinetics which describes the binding of substrate and enzyme, resulting in product formation. \\[ E + S \\leftrightarrow ES \\rightarrow E + P \\] For a more complex example, I talk about hidden markov models here.\n Works Cited Althoen, S. C., L. King, and K. Schilling. “Structural Glycobiology: A Game of Snakes and Ladders.” How Long Is A Game of Snakes and Ladders 18.8 (2008): 569. Mathematical Association. Mathematical Association, Mar. 1993. Web. 26 Sept. 2016.\n Barry, Nick. “Analysis of Chutes and Ladders.” Data Genetics. Data Genetics, 2013. Web. 26 Sept. 2016.\n Besom, Evelyn, and Sarah Rittgers. “Monte Carlo Methods with Snakes and Ladders.” Monte Carlo Method. N.p., n.d. Web. 26 Sept. 2016.\n Busa, Natalino. “Markov Chains for Ladders and Snakes.” Natalino Busa:. Blogger, 25 Jan. 2013. Web. 26 Sept. 2016.\n Campbell, J. The Maths of Snakes and Ladders (n.d.): n. pag. The Maths of Snakes and Ladders. 12 Dec. 2013. Web. 26 Sept. 2016.\n Coolen, ACC. “Markov Chains Associated With Lipschitz Kernels Examples.”Markov Chains Compact Lecture Notes and Exercises (n.d.): 63-80. King’s College London, Sept. 2009. Web. 26 Sept. 2016.\n Hochman, Michael. “CHUTES AND LADDERS.” CHUTES AND LADDERS(n.d.): n. pag. Web. 26 Sept. 2016.\n Khamsi, M. A. “Markov Chains.” Markov Chains. S.O.S. Mathematics CyberBoard, 2016. Web. 26 Sept. 2016.\n ORMethodsTutorials. “Sharkey: First Passage Times and the Chutes and Ladders Markov Chain.” YouTube. YouTube, 05 Nov. 2014. Web. 26 Sept. 2016.\n Sundar, Avinaash, and Danny Zheng. “Markov Chains - A Simple Snakes and Ladders Example.” YouTube. YouTube, 03 Aug. 2014. Web. 26 Sept. 2016.\n Wise, Barry M. “Cootie, Candyland or Chutes and Ladders: Solving a Parent’s Dilemma with Monte Carlo Simulation.” Cootie, Candyland or Chutes and Ladders: Solving a Parent’s Dilemma with Monte Carlo Simulation. Cornell University, n.d. Web. 26 Sept. 2016.\n     ","date":1582243200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582243200,"objectID":"3ed6cbeaee84a8cafb7b8b66dd386a0f","permalink":"/post/markov_chain/","publishdate":"2020-02-21T00:00:00Z","relpermalink":"/post/markov_chain/","section":"post","summary":"Markov Chain What is a markov chain?  A succession of random events.\n There is a set of states, in the above example, after coming home from school: Sleep, Homework, and Netflix.","tags":["Statistics","Modelling","Markov Chain","Python","NumPy","Transition Matrix","Absorbing Matrix","Probability","Math"],"title":"Markov Chains","type":"post"},{"authors":null,"categories":["Bioinformatics"],"content":"  Basic Programming CheatSheet Unix/Linux (Bash) Need to kill a process? Bash Script  R vs Python  Genetic Data Reference Sequence Number GRCh 37 NCBI Reference Sequence (RefSeq) Ensemble SNPs   Types of Files FASTA Fastq SAM/BAM files SDF (structure data file) VCF  Bioinformatic Resources/Tools BLAST BRENDA ExPASy PDB  Micellaneous BLOSUM (Block Storing Matrix) and PAM E.C. Number Hamming Distance K-mers Needleman-Wunsch algorithm Smith-Waterman algorithm    *** As I learn more, I will hopefully continue to add to this page for things that confused me or I thought would be helpful to compile together. As a warning, this is just a collection of notes and is not super organized.\nBasic Programming CheatSheet Unix/Linux (Bash) Another way of navigating through files and giving commands to the operating system.\nAbsolute path (begins with “/”)\n /home/Downloads/pax9.csv\n Relative path (does NOT begin with “/”)\nIf you are in /home\n Downloads\n  Downloads/pax9.csv\n If you are in /home/Downloads\n pax9.csv\n     Command Description    pwd “print working directory” (absolute)  / root directory  ls “listing”, gives contents of current directory  ls /home/Downloads contents of download directory  cd “change directory”  .. directory above current  . current directory  ~ home directory  q “quit”  Control + C “cancel”  top allows you to view the current processes running  cp test.csv test2.csv “copy”: test.csv is duplicated and named test2.csv (last arg = destination)  cp test.csv other.csv Downloads “copy”: test.csv and other.csv is copied to the Downloads directory  mv test.csv other.csv Downloads “moves”: test.csv and other.csv is moved to the downloads directory  mv test.csv new.csv “renames”: test.csv is renamed to new.csv (also works for directory)  rm test.csv other.csv “removes”: deletes other.csv and test.csv (does not work for directory)  rmdir Downloads “removes”: directory (must be empty!)  mkdir Homework “makes directory” called homework  man ls “manual” of command ls  chmod u+x test.sh “change file mode”: permissions, read ( r ), write ( w ) or execute ( x )    Useful Tricks:\n Hit up and down arrow keys to get previous commands\n Use tab key for autocompletion\n Spaces for file names can cause problems because they are seen as separate items. To prevent this, put them in quotes or use before the space\n if a file/destination does not exist, it will create one. If it does exist, it may overwrite\n there is no undo\n  Need to kill a process? Get PID (process ID) from top\n type: kill PID\n doesn’t work? type: kill -9 PID\n   Bash Script A text file with commands. Anything you put in command line can be in a script and vice versa. Uses the .sh extension.\n #!/bin/bash\n First line must always look something like this. “#!” is called a shebang and immediately after (no spaces), put the path to the interpreter. If you don’t know, type: “which bash” in command line.\n #running program from its path\n  #Bob 07/01/2020\n Comments to describe what it does, author, date, etc.\n name = “Bob”\n You can set variables\n echo Hello $name !\n Like print, it will print stuff after it. Refer to variables with a “$” in front.\n pwd\n You can put commands in the script.\nIn the actual command line, to run the script:\n ./testscript.sh\n AKA, look in the current directory to find the script named testscript.sh and run it.\n  R vs Python While R has a lot of super useful packages, especially for bioinformatics and statistics, I found it super annoying to code in base R compared to python. Here are some essential equivalents for R:\n    Python R    len(myList) length(myList) **won’t work for strings (see below example)  x in myList x %in% myList  myList.index(item) match (item, myList)  “string”[:3] substr (“string”, 1, 3)  range(0, 6,2) seq(from = 0, 4, by=2)  “str”+“ing” paste0(“str”,“ing”, \u0026quot;\u0026quot;)  myList=[1,2] myList=c(1,2)  myList.append(3) myList=c(myList, 3)  myList.extend(myList2) myList=c(myList,myList2) OR myList=append(myList, myList2)    Examples\npython: len(“string”) vs R: nchar(“string”)  for i in range(len(\u0026quot;test\u0026quot;)): print (i) ## 0 ## 1 ## 2 ## 3 for (i in 1:nchar(\u0026quot;test\u0026quot;)){ print (i) } ## [1] 1 ## [1] 2 ## [1] 3 ## [1] 4 Matrices  python\ntest_matrix=[] for i in range(3): test_matrix.append([0]*3) test_matrix[0][1]=2 print(test_matrix) ## [[0, 2, 0], [0, 0, 0], [0, 0, 0]] R\ntest_matrix\u0026lt;- matrix(0, nrow = 3, ncol = 3) test_matrix[1,2]=2 print(test_matrix) ## [,1] [,2] [,3] ## [1,] 0 2 0 ## [2,] 0 0 0 ## [3,] 0 0 0   Genetic Data Reference Sequence Number When I first started working on genetic data and especially with my PAX9 project, I was so confused by all of the different values that were associated with the gene:\nGenomic reference: NG_013357.1 (GRCh 37)\nTranscript reference: NM_006194.3\nEnsembl gene ID: ENSG00000198807.8\nCanonical transcript: ENST00000361487.6\nGRCh 37 Before going into what all of these sequences are, first of all, what is GRCh37? I kept coming across this when I first trying to find data on my gene. GRCh37 stands for Genome Reference Consortium Human Build 37 and is generally synonymous with hg19. Similarly, hg38 can also be used to refer to the same genome build as GRCh38.\nWhat is the difference between GRCh37 and GRCh38? GRCh38 was released after GRCh37 and has more updated annotations. Luckily, various tools such as from [NCBI]](https://www.ncbi.nlm.nih.gov/genome/tools/remap) or Ensembl can be used for converting annotations.\n NCBI Reference Sequence (RefSeq) The accession number format:\n NG_: gene/genomic region\n NC_: plus (+) strand on chromosome\n NT_: constructed genomic contigs (overlapping DNA fragments that are used to assemble the full sequence)\n  ‘NC’, ‘NG’, ‘NT’ all refer to genomic sequences\n NM_: mRNA, coding strand\n NR_: non-coding RNA\n NP_: protein\n  Also, if it begins with an ‘X’ liks ‘XP’ or ‘XM’, it probably means that the sequence is from computational prediction.\nWhat about CDS? That is the coding region of a gene that is translated to protein. Therefore, it excludes the UTR and introns.\n Ensemble  ENST: transcipt\n ENSG: gene (One gene can have many corresponding transcripts.)\n ENSP: protein\n   SNPs  dbSNP: Single Nucleotide Polymorphism Database from NCBI, used for SNPs\n rs: RefSNP, also used to refer to a specific SNP\n     Types of Files FASTA FASTA is a type of text-based file that is used to store a nucleotide or amino acid sequence. In R, there is the read.fasta() function from the ‘seqinr’ package. It’s written in the following format:\n \u0026gt;3LWB:A|PDBID|CHAIN|SEQUENCE MSANDRRDRRVRVAVVFGGRSNEHAISCVSAGSILRNLDSRRFDVIAVGITPAGSWVLTDANPDALTITNRELPQVKSGS GTELALPADPRRGGQLVSLPPGAGEVLESVDVVFPVLHGPYGEDGTIQGLLELAGVPYVGAGVLASAVGMDKEFTKKLLA ADGLPVGAYAVLRPPRSTLHRQECERLGLPVFVKPARGGSSIGVSRVSSWDQLPAAVARARRHDPKVIVEAAISGRELEC GVLEMPDGTLEASTLGEIRVAGVRGREDSFYDFATKYLDDAAELDVPAKVDDQVAEAIRQLAIRAFAAIDCRGLARVDFF LTDDGPVINEINTMPGFTTISMYPRMWAASGVDYPTLLATMIETTLARGVGLH \u0026gt;3LWB:B|PDBID|CHAIN|SEQUENCE MSANDRRDRRVRVAVVFGGRSNEHAISCVSAGSILRNLDSRRFDVIAVGITPAGSWVLTDANPDALTITNRELPQVKSGS GTELALPADPRRGGQLVSLPPGAGEVLESVDVVFPVLHGPYGEDGTIQGLLELAGVPYVGAGVLASAVGMDKEFTKKLLA ADGLPVGAYAVLRPPRSTLHRQECERLGLPVFVKPARGGSSIGVSRVSSWDQLPAAVARARRHDPKVIVEAAISGRELEC GVLEMPDGTLEASTLGEIRVAGVRGREDSFYDFATKYLDDAAELDVPAKVDDQVAEAIRQLAIRAFAAIDCRGLARVDFF LTDDGPVINEINTMPGFTTISMYPRMWAASGVDYPTLLATMIETTLARGVGLH\n The first line is used to describe the folowing sequence and it is indicated by the “\u0026gt;” sign. As you can see above, multiple sequences can be found in one FASTA file.\n Fastq Similar to FASTA, except from DNA sequencing (commonly for illumina). It also includes the PHRED score which is a measure of the quality of the reading for each base. It is denoted by a single ASCII character.\n @ SequenceID\n  CATGGGCAGCCGAGAGATTGCGA\n  +\n  K\u0026lt;=gux;YZ[bcs3^_`a;\u0026lt;|}\n The first line contains the description like fasta but is indicated by the “@” sign instead. After which, the nucleotide sequence is shown on the 2nd line. The third line starts with a “+” sign. Finally, the 4th line contains the quality scores.\n SAM/BAM files SAM and BAM files contain the same information, the difference is that BAM files are binary versions of a SAM file. SAM stands for “Sequence Alignment/Map” and contain information about sequences aligned to a reference sequence, and is generated by next generation sequencing.\n SDF (structure data file) ** MOL files follow the same format but are only for a single molecule while SDF files can contain multiple Three line header:\nName of Molecule (71080)\n Software to generate the file\n Comment (blank above)\n  *** Even if lines are left blank. Still need the lines there for file to function properly.\nCounts line\nNumber of atoms: 13\n Number of bonds: 12\n  Atoms (one line for each atom)\nX coordinate: 1.450\n Y coordinate: -1.0463\n Z coordinate: -0.2600\n Atom symbol: O (oxygen)\n  Bonds (one line for bond)\nIndex of atom 1\n Index of atom 2\n Type of bond (1=single, 2=double, 3=triple)\n  M END : Required at end.\nMetadata (data that provides info on other data)\n Starts with header that begins with \u0026gt;\n Name of data field is written in \u0026lt;\u0026gt;.\n E.g. \u0026gt; \u0026lt;PUBCHEM_COMPOUND_CID\u0026gt;\n  Multiple molecules are separated with four dollar signs: $$$$.\n VCF VCF stands for variant call format and stores different variants of a gene.\n  Bioinformatic Resources/Tools BLAST BLAST stands for Basic Local Alignment Search Tool. It finds areas of similarity between sequences of nucleotides or amino acids using a scoring matrix. There are many types of blast including blastn, blastp, blastx, tblastn, etc.\n Query cover: how much of the query sequence actually overlapps with the aligned sequence.\n E value (expected value): smaller is better (significant). It basically means how many hits would you get just randomly (based on the quality of score and the length of the query)\n Percent Identity: percentage of characters matching in the query and aligned sequence\n   BRENDA BRENDA contains enzyme information such as the reaction diagram, links to research papers, KM values, IC50, inhibitors for the enzyme, etc.\n ExPASy ExPASy or Expert Protein Anaylsis System can be used to find the molecular weight of a sequence of amino acids, theoretical pI, instability, extinction coefficient (for spectrophotometry), etc.\n PDB PDB or Protein Data Bank contains 3D files of mostly proteins. For each protein, it tells you the associated research paper, number of chains, any ligands present already in the structure, resolution, etc.\n  Micellaneous BLOSUM (Block Storing Matrix) and PAM Both used as a measure of similarity between proteins. PAM is for closely related while BLOSUM is for distantly related. For BLOSUM, BLOSUM45 would be used for proteins more distant from each other than BLOSUM90.\n E.C. Number You can categorize enzymes based on their EC number (Enzyme Committee number) based on the reaction they catalyze. For example, D-alanine D-alanine ligase has an E.C. number of 6.3.2.4. The “6” indicates that it is a ligase (joining two molecules using ATP).\n Hamming Distance What is the minimum number of nucleotides that need to be changed for one string to transform into another (if they are the same length). In biology, one application is in identification: in illumina sequencing, you can sequence multiple samples together and they can be differentiated due to adaptor sequences which have barcode/index sequences that are unique to each sample.\n     Sequence 1 Sequence 2 (Hamming dist=1) Sequence 3 (Hamming dist=2) Sequence 3 (Hamming dist=2)    Original ACCG ACTG ACTT GCTT  1 Mutation “did not change” ACCG ACCT GCCT  2 Mutations “did not change” - ACCG GCCT  3 Mutations “did not change” - - ACCG    However, if one mutation occurs per sequence, than you need at least a hamming distance of 2(error)+1. For example: if you’re starting sequences are “ACCG” and “GCTT”, what if you get the sequence “ACCT”? Is that the sequence “ACCG”–\u0026gt; “ACCT”? Or “GCCT”–\u0026gt; “ACCT”? Therefore, if you have one error per sequence, you need a hamming distance of atleast 3 .\n K-mers Subsequences of length k of a nucleotide sequence.\ndef findkmers(k, seq): dict1={} a = 0 b= k k_mer= seq[a:b] while len(k_mer)==k: if k_mer in dict1: dict1[k_mer]+= 1 else: dict1[k_mer]= 1 a+=1 b+=1 k_mer= seq[a:b] print (dict1) def main(): my_seq1 = \u0026quot;CAGCCCAATC\u0026quot; print(\u0026quot;For the sequence: \u0026quot;, my_seq1) for i in range (1,5): print(\u0026quot;The\u0026quot;, str(i)+\u0026quot;-mers are :\u0026quot;) findkmers(i, my_seq1) main() ## For the sequence: CAGCCCAATC ## The 1-mers are : ## {\u0026#39;C\u0026#39;: 5, \u0026#39;A\u0026#39;: 3, \u0026#39;G\u0026#39;: 1, \u0026#39;T\u0026#39;: 1} ## The 2-mers are : ## {\u0026#39;CA\u0026#39;: 2, \u0026#39;AG\u0026#39;: 1, \u0026#39;GC\u0026#39;: 1, \u0026#39;CC\u0026#39;: 2, \u0026#39;AA\u0026#39;: 1, \u0026#39;AT\u0026#39;: 1, \u0026#39;TC\u0026#39;: 1} ## The 3-mers are : ## {\u0026#39;CAG\u0026#39;: 1, \u0026#39;AGC\u0026#39;: 1, \u0026#39;GCC\u0026#39;: 1, \u0026#39;CCC\u0026#39;: 1, \u0026#39;CCA\u0026#39;: 1, \u0026#39;CAA\u0026#39;: 1, \u0026#39;AAT\u0026#39;: 1, \u0026#39;ATC\u0026#39;: 1} ## The 4-mers are : ## {\u0026#39;CAGC\u0026#39;: 1, \u0026#39;AGCC\u0026#39;: 1, \u0026#39;GCCC\u0026#39;: 1, \u0026#39;CCCA\u0026#39;: 1, \u0026#39;CCAA\u0026#39;: 1, \u0026#39;CAAT\u0026#39;: 1, \u0026#39;AATC\u0026#39;: 1}  Needleman-Wunsch algorithm Global alignment\n Smith-Waterman algorithm Local alignment\nReferences\nhttp://useast.ensembl.org/info/website/tutorials/grch37.html\n https://bitesizebio.com/38335/get-to-know-your-reference-genome-grch37-vs-grch38/\n https://www.ncbi.nlm.nih.gov/books/NBK50679/#RefSeqFAQ.what_is_a_reference_sequence_r\n https://thesequencingcenter.com/knowledge-base/fastq-files/\n Kerfeld and Scott, PLoS Biology 2011\n https://ryanstutorials.net/linuxtutorial/\n https://ryanstutorials.net/bash-scripting-tutorial/bash-script.php\n https://learn.datacamp.com/\n http://www.nonlinear.com/progenesis/sdf-studio/v0.9/faq/sdf-file-format-guidance.aspx\n    ","date":1578614400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578614400,"objectID":"c6c67ad839cb8ea7671484839df4e569","permalink":"/post/bioinformatics_glossary/","publishdate":"2020-01-10T00:00:00Z","relpermalink":"/post/bioinformatics_glossary/","section":"post","summary":"What do all the acronyms like 'NG_', 'NM_', 'ENSG', etc. mean? What are all these different files (FASTA, VCF, etc.) used for? Basics of UNIX? R vs Python equivalents? And more !","tags":["Bioinformatics"],"title":"Bioinformatics Glossary","type":"post"},{"authors":null,"categories":["Statistics"],"content":" Modeling Introduction This dataset is comprised of a merged dataset from World Health Organization and The World Bank Group. It consists of information about cases of Tuberculosis including new pulmonary cases, previous cases, drug resistant cases (MDR and XDR), mortality due to tuberculosis, etc. This dataset also contains information about country, region, year (2017 or 2018), population number, and GDP per capita in US dollars.\n MANOVA testing  #H0: For each response variable, the means of the groups are equal #H1: For at least one response variable, at least one group mean differs head(TB_Data) ## country region year new.pul.TB prev.treated.pul.TB prev.unk.pul.TB new.MDR prev.MDR ## 1 Albania EUR 2017 195 15 0 0 0 ## 2 Algeria AFR 2017 6278 419 0 11 28 ## 3 Algeria AFR 2018 6137 362 21 2 5 ## 4 American Samoa WPR 2017 3 0 0 1 0 ## 5 Andorra EUR 2017 1 0 0 0 0 ## 6 Angola AFR 2018 31098 6623 0 0 0 ## MDR.tested XDR pop.number TB.100k TB.num TB_mort.100k TB_mort.num GDP ## 1 0 0 2884169 20.0 580 0.34 10 4532.889 ## 2 39 4 41389189 70.0 29000 7.70 3200 4048.285 ## 3 7 4 42228408 69.0 29000 7.70 3300 4278.850 ## 4 1 0 55620 10.0 6 0.85 0 11398.777 ## 5 0 0 77001 1.5 1 0.12 0 39134.393 ## 6 0 0 30809787 355.0 109000 72.00 22000 3432.386 man1\u0026lt;-manova(cbind(new.pul.TB,prev.treated.pul.TB,prev.unk.pul.TB, new.MDR, prev.MDR, XDR, pop.number, TB.100k, TB_mort.100k, GDP)~region, data=TB_Data) summary(man1) ## Df Pillai approx F num Df den Df Pr(\u0026gt;F) ## region 5 1.2666 7.1585 50 1055 \u0026lt; 2.2e-16 *** ## Residuals 216 ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 There are alot of assumptions for the MANOVA test which indicates that they are usually hard to test and/or meet. These assumptions include: random samples, independent observations, dependent variables have multivariate normality, homogeneity of within-group covariance matrices for each dependent variable and equal covariance between any two dependent variables, linear relationships amoung dependent variables, no extreme univariate or multivariate outliers, and no multicollinearity. The dataset would likely not have been able to meet the assumptions, especially since many of the variables are likely correlated (e.g.: new drug resistant TB cases is likely related to previous/new TB cases ).\nFor the MANOVA, only 10 dependent variables were chosen. For example, only one of TB.100k (TB cases per 100,000 people) and TB.num (TB cases total) since they are likely to be highly related. The p-value (\u0026lt; 2.2e-16) is significant for the MANOVA test, therefore, for at least one of the response variables, the mean between regions is different.\nUnivariate ANOVA testing summary.aov(man1) ## Response new.pul.TB : ## Df Sum Sq Mean Sq F value Pr(\u0026gt;F) ## region 5 3.7015e+11 7.4029e+10 13.356 2.385e-11 *** ## Residuals 216 1.1972e+12 5.5427e+09 ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Response prev.treated.pul.TB : ## Df Sum Sq Mean Sq F value Pr(\u0026gt;F) ## region 5 1.1311e+10 2262181943 7.6302 1.268e-06 *** ## Residuals 216 6.4039e+10 296477285 ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Response prev.unk.pul.TB : ## Df Sum Sq Mean Sq F value Pr(\u0026gt;F) ## region 5 30299516 6059903 3.4395 0.005189 ** ## Residuals 216 380558289 1761844 ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Response new.MDR : ## Df Sum Sq Mean Sq F value Pr(\u0026gt;F) ## region 5 4062809 812562 1.8615 0.1023 ## Residuals 216 94285428 436507 ## ## Response prev.MDR : ## Df Sum Sq Mean Sq F value Pr(\u0026gt;F) ## region 5 12235002 2447000 1.9446 0.08818 . ## Residuals 216 271810715 1258383 ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Response XDR : ## Df Sum Sq Mean Sq F value Pr(\u0026gt;F) ## region 5 468938 93788 1.388 0.2299 ## Residuals 216 14595646 67572 ## ## Response pop.number : ## Df Sum Sq Mean Sq F value Pr(\u0026gt;F) ## region 5 7.5761e+17 1.5152e+17 10.661 3.61e-09 *** ## Residuals 216 3.0700e+18 1.4213e+16 ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Response TB.100k : ## Df Sum Sq Mean Sq F value Pr(\u0026gt;F) ## region 5 1375556 275111 22.228 \u0026lt; 2.2e-16 *** ## Residuals 216 2673438 12377 ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Response TB_mort.100k : ## Df Sum Sq Mean Sq F value Pr(\u0026gt;F) ## region 5 81943 16388.6 23.624 \u0026lt; 2.2e-16 *** ## Residuals 216 149845 693.7 ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Response GDP : ## Df Sum Sq Mean Sq F value Pr(\u0026gt;F) ## region 5 2.0424e+10 4084759281 15.597 4.243e-13 *** ## Residuals 216 5.6568e+10 261891068 ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 Of the 10 dependent variables tested, the univariate ANOVAs with a p-value \u0026lt;0.05 include: new.pul.TB, prev.treated.pul.TB, prev.unk.pul.TB, pop.number, TB.100k, TB_mort.100k, and GDP.\n Post hoc t-testing # region in world: AFR=Africa; AMR=Americas; # EMR=Eastern Mediterranean; EUR=Europe; SEAR=South-East # Asia; WPR=Western Pacific #New pulmonary tuberculosis cases pairwise.t.test(TB_Data$new.pul.TB,TB_Data$region,p.adj=\u0026quot;none\u0026quot;) ## ## Pairwise comparisons using t tests with pooled SD ## ## data: TB_Data$new.pul.TB and TB_Data$region ## ## AFR AMR EMR EUR SEA ## AMR 0.71 - - - - ## EMR 0.83 0.60 - - - ## EUR 0.64 0.96 0.54 - - ## SEA 2.1e-11 7.2e-12 2.5e-09 3.8e-13 - ## WPR 0.80 0.92 0.67 0.87 4.7e-11 ## ## P value adjustment method: none #Previously treated pulmonary tuberculosis cases pairwise.t.test(TB_Data$prev.treated.pul.TB,TB_Data$region,p.adj=\u0026quot;none\u0026quot;) ## ## Pairwise comparisons using t tests with pooled SD ## ## data: TB_Data$prev.treated.pul.TB and TB_Data$region ## ## AFR AMR EMR EUR SEA ## AMR 0.88 - - - - ## EMR 0.90 0.80 - - - ## EUR 0.98 0.89 0.87 - - ## SEA 1.3e-07 1.0e-07 2.4e-06 3.1e-08 - ## WPR 0.96 0.93 0.87 0.98 3.7e-07 ## ## P value adjustment method: none #Confirmed pulmonary TB cases with unknown TB treatment history pairwise.t.test(TB_Data$prev.unk.pul.TB,TB_Data$region,p.adj=\u0026quot;none\u0026quot;) ## ## Pairwise comparisons using t tests with pooled SD ## ## data: TB_Data$prev.unk.pul.TB and TB_Data$region ## ## AFR AMR EMR EUR SEA ## AMR 0.97315 - - - - ## EMR 0.98235 0.99463 - - - ## EUR 0.74157 0.77669 0.80590 - - ## SEA 0.00023 0.00031 0.00090 0.00028 - ## WPR 0.98122 0.95689 0.96726 0.74643 0.00042 ## ## P value adjustment method: none #Population Number pairwise.t.test(TB_Data$pop.number,TB_Data$region,p.adj=\u0026quot;none\u0026quot;) ## ## Pairwise comparisons using t tests with pooled SD ## ## data: TB_Data$pop.number and TB_Data$region ## ## AFR AMR EMR EUR SEA ## AMR 1.00 - - - - ## EMR 0.61 0.62 - - - ## EUR 0.92 0.93 0.64 - - ## SEA 5.7e-10 9.3e-10 1.2e-07 1.5e-10 - ## WPR 0.76 0.77 0.46 0.68 8.6e-10 ## ## P value adjustment method: none #TB cases per 100,000 individuals pairwise.t.test(TB_Data$TB.100k,TB_Data$region,p.adj=\u0026quot;none\u0026quot;) ## ## Pairwise comparisons using t tests with pooled SD ## ## data: TB_Data$TB.100k and TB_Data$region ## ## AFR AMR EMR EUR SEA ## AMR 3.4e-12 - - - - ## EMR 4.2e-06 0.11090 - - - ## EUR 1.2e-15 0.72536 0.04362 - - ## SEA 0.72362 7.4e-07 0.00082 4.6e-08 - ## WPR 0.02843 6.2e-06 0.01175 1.5e-07 0.18283 ## ## P value adjustment method: none #TB mortality cases per 100,000 individuals pairwise.t.test(TB_Data$TB_mort.100k,TB_Data$region,p.adj=\u0026quot;none\u0026quot;) ## ## Pairwise comparisons using t tests with pooled SD ## ## data: TB_Data$TB_mort.100k and TB_Data$region ## ## AFR AMR EMR EUR SEA ## AMR 2.3e-15 - - - - ## EMR 9.7e-10 0.34679 - - - ## EUR \u0026lt; 2e-16 0.74776 0.19773 - - ## SEA 0.00066 0.00411 0.05824 0.00109 - ## WPR 1.2e-10 0.21678 0.85228 0.09768 0.06676 ## ## P value adjustment method: none #GDP pairwise.t.test(TB_Data$GDP,TB_Data$region,p.adj=\u0026quot;none\u0026quot;) ## ## Pairwise comparisons using t tests with pooled SD ## ## data: TB_Data$GDP and TB_Data$region ## ## AFR AMR EMR EUR SEA ## AMR 0.03154 - - - - ## EMR 0.00053 0.10499 - - - ## EUR 1.0e-13 2.1e-07 0.00652 - - ## SEA 0.82314 0.16977 0.01100 2.6e-07 - ## WPR 0.00045 0.13530 0.81059 0.00098 0.01345 ## ## P value adjustment method: none  Bonferroni correction #MANOVA: 1 #ANOVA: 11 #T-tests: 15* 7= 105 #Probability of a type I error if unadjusted 1-(1-0.05)^117 ## [1] 0.9975245 #New alpha 0.05/(1+10+105) ## [1] 0.0004310345 The total number of tests conducted is 117, therefore if unadjusted, the probability of at least one type I error is 0.9975. Therefore, the bonferroni correction was used to obtain the new alpha value of 0.00043 so that the overall alpha value remains at 0.05.\nUsing this new alpha value, the MANOVA is still significant. The ANOVAs of new.pul.TB, prev.treated.pul.TB, pop.number, TB.100k, TB_mort.100k, and GDP are still significant. Only the ANOVA of prev.unk.pul.TB is no longer significant (p-value=0.005189). For the post-hoc analysis of the new pulmonary TB cases, previously treated pulmonary TB, and population number only the means of SEA are different from the other regions. For TB.100k there are significant differences between AFR/AMR, AFR/EMR, AFR/EUR, AMR/SEA, AMR/WPR, SEA/EMR, EUR/SEA, and EUR/WPR. For the TB mortality for every 100,000 individuals, AFR is significantly different from the other regions. For GDP, AFR/EMR, AFR/EUR, AFR/WPR, AMR/EUR, EUR/SEA, and EUR/WPR are significantly different.\n  Randomization test: PERMANOVA #H0: The centroid and dispersion of points is the same for the different regions #H1: The centroid and dispersion of points is different for at least one of the different regions #compute distances/dissimilarities dists\u0026lt;-TB_Data%\u0026gt;%dplyr::select(new.pul.TB,prev.treated.pul.TB,prev.unk.pul.TB, new.MDR, prev.MDR, XDR, pop.number, TB.100k, TB_mort.100k, GDP)%\u0026gt;%dist() #compute observed F SST\u0026lt;- sum(dists^2)/150 SSW\u0026lt;-TB_Data%\u0026gt;%group_by(region)%\u0026gt;%dplyr::select(new.pul.TB,prev.treated.pul.TB,prev.unk.pul.TB, new.MDR, prev.MDR, XDR, pop.number, TB.100k, TB_mort.100k, GDP)%\u0026gt;% do(d=dist(.[2:3],\u0026quot;euclidean\u0026quot;))%\u0026gt;%ungroup()%\u0026gt;% summarise(sum(d[[1]]^2)/50 + sum(d[[2]]^2)/50+ sum(d[[3]]^2)/50)%\u0026gt;%pull F_obs\u0026lt;-((SST-SSW)/2)/(SSW/147) #observed F statistic F_obs ## [1] 21739142596 # compute null distribution for F Fs\u0026lt;-replicate(1000,{ new\u0026lt;-TB_Data%\u0026gt;%mutate(region=sample(region)) #permute the region vector SSW\u0026lt;-new%\u0026gt;%group_by(region)%\u0026gt;%dplyr::select(new.pul.TB,prev.treated.pul.TB,prev.unk.pul.TB, new.MDR, prev.MDR, XDR, pop.number, TB.100k, TB_mort.100k, GDP)%\u0026gt;% do(d=dist(.[2:3],\u0026quot;euclidean\u0026quot;))%\u0026gt;%ungroup()%\u0026gt;% summarise(sum(d[[1]]^2)/50 + sum(d[[2]]^2)/50+ sum(d[[3]]^2)/50)%\u0026gt;%pull ((SST-SSW)/2)/(SSW/147) #calculate new F on randomized data }) {hist(Fs,prob = T); abline(v=F_obs, col=\u0026quot;red\u0026quot;, add=T)} mean(Fs\u0026gt;F_obs) #p-value  ## [1] 0.01 Since the data is unlikely to meet the assumptions of the MANOVA, a randomization-test MANOVA (PERMANOVA) was conducted which resulted in a p-value \u0026lt; 0.05 (significant). Therefore the null hypothesis is rejected in favor of the alternative hypothesis that there is a significant difference between the centroid and/or the spread of the points is not equal between the different regions. This therefore supports the conclusion of the MANOVA conducted previously.\n Linear regression model TB_Data$GDP_c \u0026lt;- TB_Data$GDP - mean(TB_Data$GDP, na.rm = T) TB_Data$TB_mort.100k_c \u0026lt;- TB_Data$TB_mort.100k - mean(TB_Data$TB_mort.100k, na.rm = T) fit1\u0026lt;-lm(TB_mort.100k_c ~ GDP_c*region, data=TB_Data) #uncorrected SEs summary(fit1) ## ## Call: ## lm(formula = TB_mort.100k_c ~ GDP_c * region, data = TB_Data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -54.036 -5.622 -1.086 1.755 144.129 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 1.059e+00 1.514e+01 0.070 0.9443 ## GDP_c -2.858e-03 1.179e-03 -2.423 0.0162 * ## regionAMR -1.472e+01 1.595e+01 -0.923 0.3572 ## regionEMR -6.498e+00 1.605e+01 -0.405 0.6860 ## regionEUR -1.457e+01 1.557e+01 -0.936 0.3505 ## regionSEA -3.136e+01 3.331e+01 -0.942 0.3475 ## regionWPR -5.678e+00 1.582e+01 -0.359 0.7200 ## GDP_c:regionAMR 2.651e-03 1.328e-03 1.996 0.0472 * ## GDP_c:regionEMR 2.457e-03 1.211e-03 2.029 0.0437 * ## GDP_c:regionEUR 2.789e-03 1.188e-03 2.349 0.0198 * ## GDP_c:regionSEA -6.893e-04 2.812e-03 -0.245 0.8066 ## GDP_c:regionWPR 2.541e-03 1.204e-03 2.110 0.0361 * ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 25.98 on 210 degrees of freedom ## Multiple R-squared: 0.3884, Adjusted R-squared: 0.3564 ## F-statistic: 12.12 on 11 and 210 DF, p-value: \u0026lt; 2.2e-16 For the model with uncorrected standard errors, controlling for region, for every increase of 1 USD of GDP from the mean, the number of cases of mortality due to tuberculosis per 100,000 people decreases by 2.858e-03. Compared to the reference region of AFR and controlling for GDP, the mean number of cases of mortality due to tuberculosis per 100,000 people decreases for all regions: AMR (1.472e+01), EMR(6.498e+00), EUR (1.457e+01), SEA (3.136e+01), and WPR (5.678e+00). The coefficient for the interactions describe the difference in the effect of GDP on the mortality due to tuberculosis per 100,00 people depending on which region the country is in (the slope difference between the regions). For example, the slope difference between AFR and: AMR (2.651e-03), EMR (2.457e-03), EUR (2.789e-03), SEA (-6.893e-04), and WPR (2.541e-03).\nPlot the regression TB_Data%\u0026gt;%ggplot(aes(x=GDP_c, y=TB_mort.100k_c,group=region))+geom_point(aes(color=region))+geom_smooth(method = \u0026#39;lm\u0026#39;,se=F, aes(color=region))  Asumptions #Linear relationship between each predictor and response (no scatterplot for region since it is categorical) ggplot(TB_Data, aes(x=GDP_c, y=TB_mort.100k_c,group=region))+geom_point(aes(color=region)) # Confirm normality of residuals qqnorm(fit1$residuals, main = \u0026quot;QQ-plot of Model Residuals\u0026quot;) qqline(fit1$residuals, col = \u0026quot;red\u0026quot;) # Confirm equal variance plot(fit1$fitted.values, fit1$residuals, xlab = \u0026quot;Fitted Values\u0026quot;, ylab = \u0026quot;Residuals\u0026quot;, pch = 20) abline(h = 0, col = \u0026quot;red\u0026quot;) library(sandwich); library(lmtest) bptest(fit1) ## ## studentized Breusch-Pagan test ## ## data: fit1 ## BP = 58.313, df = 11, p-value = 1.905e-08 The data fails all assumptions. The scatterplot does not show a linear relationship between GDP and TB_mort.100k. The QQ-plot of the model residuals is also not normal. There is also a funneling pattern in the residual plot which indicates that it does not meet the assumption of equal variance (homoskedasticity). This is again confirmed by the Breusch-Pagan test which concluded that the homoskedasticity assumption is not met (p-value=1.905e-08).\n Recompute regression results with robust standard errors #robust SEs coeftest(fit1, vcov = vcovHC(fit1)) ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 1.0589e+00 2.0328e+01 0.0521 0.95851 ## GDP_c -2.8577e-03 1.7036e-03 -1.6775 0.09494 . ## regionAMR -1.4716e+01 2.0333e+01 -0.7238 0.47001 ## regionEMR -6.4979e+00 2.0762e+01 -0.3130 0.75462 ## regionEUR -1.4572e+01 2.0334e+01 -0.7166 0.47441 ## regionSEA -3.1361e+01 2.2356e+01 -1.4028 0.16215 ## regionWPR -5.6775e+00 2.0456e+01 -0.2776 0.78163 ## GDP_c:regionAMR 2.6510e-03 1.7052e-03 1.5546 0.12155 ## GDP_c:regionEMR 2.4566e-03 1.7146e-03 1.4327 0.15342 ## GDP_c:regionEUR 2.7895e-03 1.7036e-03 1.6374 0.10305 ## GDP_c:regionSEA -6.8927e-04 1.9443e-03 -0.3545 0.72331 ## GDP_c:regionWPR 2.5408e-03 1.7056e-03 1.4897 0.13781 ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 A model with robust standard errors should be used if the data is heteroskedastic. With uncorrected standard errors, the coefficient estimates of the main effect of GDP (p-value = 0.0162) was significant. The interactions GDP_c:regionAMR (p-value= 0.0472), GDP_c:regionEMR (p-value= 0.0437), GDP_c:regionEUR (p-value= 0.0198), and GDP_c:regionWPR (p-value= 0.0361) were also significant. Contrastingly, using the more robust SEs (more conservative), none of the coefficient estimates are still significant.\n Effect Size summary(fit1)$adj.r.squared ## [1] 0.3563608 The adjusted r-squared of 0.356 indicates that 35.6% of variation in the mean number of cases of mortality due to tuberculosis per 100,000 people is explained by the model.\n Regression but without interactions fit2\u0026lt;-lm(TB_mort.100k_c ~ GDP_c+region, data=TB_Data) summary(fit2) ## ## Call: ## lm(formula = TB_mort.100k_c ~ GDP_c + region, data = TB_Data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -49.946 -6.211 -1.696 2.397 146.834 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 3.394e+01 4.177e+00 8.125 3.49e-14 *** ## GDP_c -2.055e-04 1.101e-04 -1.866 0.063421 . ## regionAMR -4.759e+01 5.783e+00 -8.230 1.80e-14 *** ## regionEMR -3.978e+01 6.834e+00 -5.822 2.10e-08 *** ## regionEUR -4.572e+01 5.796e+00 -7.888 1.53e-13 *** ## regionSEA -2.635e+01 7.646e+00 -3.446 0.000684 *** ## regionWPR -3.867e+01 6.261e+00 -6.177 3.23e-09 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 26.19 on 215 degrees of freedom ## Multiple R-squared: 0.3638, Adjusted R-squared: 0.3461 ## F-statistic: 20.49 on 6 and 215 DF, p-value: \u0026lt; 2.2e-16 Examining only the main effects, the effect of GDP is no longer significant while all the coeficient estimates for region are now significant.\n Linear regression model with bootstrapped standard errors # repeat 5000 times, saving the coefficients each time samp_distn\u0026lt;-replicate(5000, { boot_TB\u0026lt;-TB_Data[sample(nrow(TB_Data),replace=TRUE),] fit3\u0026lt;-lm(TB_mort.100k_c ~ GDP_c*region, data=boot_TB) coef(fit3) }) #Estimated SEs samp_distn%\u0026gt;%t%\u0026gt;%as.data.frame%\u0026gt;%summarise_all(sd) ## (Intercept) GDP_c regionAMR regionEMR regionEUR regionSEA regionWPR GDP_c:regionAMR ## 1 38.5149 0.003056803 38.5189 38.75788 38.51718 41.48764 38.50866 0.003058287 ## GDP_c:regionEMR GDP_c:regionEUR GDP_c:regionSEA GDP_c:regionWPR ## 1 0.0030693 0.00305682 0.003327013 0.003059335 Bootstrapped standard errors are used when the data is non-normal as well. Compared to the original standard errors, the standard errors for all the coefficient estimates increased (more conservative). The p-values are therefore higher as well because of the higher standard errors. Compared to the robust standard errors, the bootstrapped standard errors are also higher. Therefore, the p-values are also higher (more conservative).\n  Logistic regression predicting a binary categorical variable BinaryTB\u0026lt;-TB_Data%\u0026gt;%mutate(y = as.numeric(ifelse(XDR \u0026gt; 0, 1, 0))) fit4\u0026lt;-glm(y~region+TB.100k+pop.number,data=BinaryTB,family=binomial(link=\u0026quot;logit\u0026quot;)) coeftest(fit4) ## ## z test of coefficients: ## ## Estimate Std. Error z value Pr(\u0026gt;|z|) ## (Intercept) -2.5554e+00 5.5591e-01 -4.5968 4.291e-06 *** ## regionAMR 2.2980e-01 6.5681e-01 0.3499 0.7264340 ## regionEMR 1.2454e+00 6.4541e-01 1.9296 0.0536624 . ## regionEUR 2.2567e+00 5.6757e-01 3.9761 7.005e-05 *** ## regionSEA 4.0806e-01 8.6513e-01 0.4717 0.6371581 ## regionWPR -2.4001e-01 6.4077e-01 -0.3746 0.7079782 ## TB.100k 5.4362e-03 1.5506e-03 3.5059 0.0004551 *** ## pop.number 3.8675e-08 1.0182e-08 3.7984 0.0001456 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 exp(coef(fit4)) ## (Intercept) regionAMR regionEMR regionEUR regionSEA regionWPR TB.100k pop.number ## 0.07766039 1.25834993 3.47417743 9.55178621 1.50390035 0.78661626 1.00545098 1.00000004 A binary variable was created that measures whether there are any XDR (extensively drug resistant tuberculosis) cases in a country. Controlling for TB.100k and population number, the odds of having a XDR TB case are 1.25 times higher for the region AMR than AFR. The odds for the other regions compared to AFR: EMR (3.47), EUR (9.55), SEA (1.50), WPR (0.79). The region with the lowest odds of a country having a case of XDR TB is WPR. The region that has the highest odds of a country having a case of XDR TB is EUR. Controlling for region and TB.100k, for every 1-unit increase in population number, odds of having a case of XDR TB change by a factor of 1.00000004. Controlling for region and population number, for every 1-unit increase in TB.100k, odds of having a case of XDR TB change by a factor of 1.00545098 (increase by 0.54%).\nConfusion Matrix prob \u0026lt;- predict(fit4, type = \u0026quot;response\u0026quot;) #get predictions pred \u0026lt;- ifelse(prob \u0026gt; 0.5, 1, 0) table(truth = BinaryTB$y, prediction = pred) %\u0026gt;% addmargins ## prediction ## truth 0 1 Sum ## 0 109 23 132 ## 1 34 56 90 ## Sum 143 79 222 #Classification Function class_diag\u0026lt;-function(probs,truth){ tab\u0026lt;-table(factor(probs\u0026gt;.5,levels=c(\u0026quot;FALSE\u0026quot;,\u0026quot;TRUE\u0026quot;)),truth) acc=sum(diag(tab))/sum(tab) sens=tab[2,2]/colSums(tab)[2] spec=tab[1,1]/colSums(tab)[1] ppv=tab[2,2]/rowSums(tab)[2] if(is.numeric(truth)==FALSE \u0026amp; is.logical(truth)==FALSE) truth\u0026lt;-as.numeric(truth)-1 #CALCULATE EXACT AUC ord\u0026lt;-order(probs, decreasing=TRUE) probs \u0026lt;- probs[ord]; truth \u0026lt;- truth[ord] TPR=cumsum(truth)/max(1,sum(truth)) FPR=cumsum(!truth)/max(1,sum(!truth)) dup\u0026lt;-c(probs[-1]\u0026gt;=probs[-length(probs)], FALSE) TPR\u0026lt;-c(0,TPR[!dup],1); FPR\u0026lt;-c(0,FPR[!dup],1) n \u0026lt;- length(TPR) auc\u0026lt;- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) ) data.frame(acc,sens,spec,ppv,auc) } class_diag(prob, BinaryTB$y) ## acc sens spec ppv auc ## 1 0.7432432 0.6222222 0.8257576 0.7088608 0.8367845 From the confusion matrix, the model has an accuracy of 0.743 which is fair. The sensitivity or true positive rate is 0.622 which is poor. The specificity or true negative rate is 0.826 which is good. The precision or positive predictive value of the model is 0.709 which is fair.\n Density of log-odds (logit) BinaryTB$logit\u0026lt;-predict(fit4) #get predicted log-odds BinaryTB$factor.y\u0026lt;-factor(BinaryTB$y,levels=c(1,0)) ggplot(BinaryTB,aes(logit, fill=factor.y))+geom_density(alpha=.3)+ geom_vline(xintercept=0,lty=2) #### ROC Curve\nlibrary(plotROC) prob \u0026lt;- predict(fit4, type = \u0026quot;response\u0026quot;) ROCplot1 \u0026lt;- ggplot(BinaryTB) + geom_roc(aes(d = y, m = prob),n.cuts = 0)+geom_segment(aes(x=0,xend=1,y=0,yend=1),lty=2) ROCplot1 calc_auc(ROCplot1) ## PANEL group AUC ## 1 1 -1 0.8367845 The AUC is determined to be 0.837 which suggests that the model is good at predicting whether a country has a case of XDR TB (the probability that a randomly selected country with a case of XDR TB has a higher prediction than a randomly selected country without a case of XDR TB).\n  10-fold CV k = 10 data1 \u0026lt;- BinaryTB[sample(nrow(BinaryTB)), ] #put dataset in random order folds \u0026lt;- cut(seq(1:nrow(BinaryTB)), breaks = k, labels = F) #create folds diags \u0026lt;- NULL for (i in 1:k) { # FOR EACH OF 10 FOLDS train \u0026lt;- data1[folds != i, ] #CREATE TRAINING SET test \u0026lt;- data1[folds == i, ] #CREATE TESTING SET truth \u0026lt;- test$y fit \u0026lt;- glm(y~region+TB.100k+pop.number,data=BinaryTB,family=binomial(link=\u0026quot;logit\u0026quot;)) probs \u0026lt;- predict(fit, newdata = test, type = \u0026quot;response\u0026quot;) diags \u0026lt;- rbind(diags, class_diag(probs, truth)) } apply(diags, 2, mean) #AVERAGE THE DIAGNOSTICS ACROSS THE 10 FOLDS ## acc sens spec ppv auc ## 0.7434783 0.6364087 0.8170467 0.7267569 0.8221299 The out of sample confusion matrix for this model has: accuracy (0.742- fair), sensitivity (0.625- poor), specificity (0.811- good), precision (0.692- poor). Although, precision decreased from fair to poor, the classifications did not change greatly. The AUC also did not change significantly which suggests that there was not overfitting.\n LASSO (least absolute shrinkage and selection operator) Regression head(BinaryTB) ## country region year new.pul.TB prev.treated.pul.TB prev.unk.pul.TB new.MDR prev.MDR ## 1 Albania EUR 2017 195 15 0 0 0 ## 2 Algeria AFR 2017 6278 419 0 11 28 ## 3 Algeria AFR 2018 6137 362 21 2 5 ## 4 American Samoa WPR 2017 3 0 0 1 0 ## 5 Andorra EUR 2017 1 0 0 0 0 ## 6 Angola AFR 2018 31098 6623 0 0 0 ## MDR.tested XDR pop.number TB.100k TB.num TB_mort.100k TB_mort.num GDP GDP_c ## 1 0 0 2884169 20.0 580 0.34 10 4532.889 -10208.887 ## 2 39 4 41389189 70.0 29000 7.70 3200 4048.285 -10693.491 ## 3 7 4 42228408 69.0 29000 7.70 3300 4278.850 -10462.926 ## 4 1 0 55620 10.0 6 0.85 0 11398.777 -3342.999 ## 5 0 0 77001 1.5 1 0.12 0 39134.393 24392.617 ## 6 0 0 30809787 355.0 109000 72.00 22000 3432.386 -11309.391 ## TB_mort.100k_c y logit factor.y ## 1 -16.129009 0 -0.07841423 0 ## 2 -8.769009 1 -0.57416872 1 ## 3 -8.769009 1 -0.54714847 1 ## 4 -15.619009 0 -2.73891183 0 ## 5 -16.349009 0 -0.28754952 0 ## 6 55.530991 0 0.56598835 0 LassoTB\u0026lt;-BinaryTB%\u0026gt;%dplyr::select(region,year,new.pul.TB, prev.treated.pul.TB, prev.unk.pul.TB,new.MDR, prev.MDR, MDR.tested,pop.number, TB.100k, TB.num, TB_mort.100k, TB_mort.num,GDP,y) fit5 \u0026lt;- glm(y~.,data=LassoTB,family=binomial(link=\u0026quot;logit\u0026quot;)) library(glmnet) x \u0026lt;- model.matrix(fit5)[, -1] y \u0026lt;- as.matrix(LassoTB$y) cv \u0026lt;- cv.glmnet(x, y, family = \u0026quot;binomial\u0026quot;) lasso \u0026lt;- glmnet(x, y, family = \u0026quot;binomial\u0026quot;, lambda = cv$lambda.1se) coef(lasso) ## 19 x 1 sparse Matrix of class \u0026quot;dgCMatrix\u0026quot; ## s0 ## (Intercept) -1.184453e+00 ## regionAMR -9.067621e-03 ## regionEMR 4.794832e-01 ## regionEUR 1.507004e+00 ## regionSEA 3.001555e-01 ## regionWPR -2.811042e-01 ## year . ## new.pul.TB . ## prev.treated.pul.TB . ## prev.unk.pul.TB . ## new.MDR 6.021496e-04 ## prev.MDR . ## MDR.tested . ## pop.number 6.064149e-09 ## TB.100k 2.758181e-03 ## TB.num . ## TB_mort.100k . ## TB_mort.num . ## GDP -1.286157e-05 Variables with a non-zero coefficient from LASSO will be retained in the model: region, new.MDR, pop.number, TB.100k, and GDP. Therefore, these are the most important predictors of if a country has atleast one case of XDR TB.\nPerform 10-fold CV using LASSO model #Model with just non-zero lasso coefficient estimates fit6 \u0026lt;- glm(y ~ region+new.MDR+pop.number+TB.100k+GDP, data = LassoTB, family = \u0026quot;binomial\u0026quot;) prob6 \u0026lt;- predict(fit6, type = \u0026quot;response\u0026quot;) class_diag(prob6, LassoTB$y) ## acc sens spec ppv auc ## 1 0.7972973 0.6333333 0.9090909 0.826087 0.8883838 #10-fold CV k = 10 data1 \u0026lt;- LassoTB[sample(nrow(LassoTB)), ] #put dataset in random order folds \u0026lt;- cut(seq(1:nrow(LassoTB)), breaks = k, labels = F) #create folds diags \u0026lt;- NULL for (i in 1:k) { # FOR EACH OF 10 FOLDS train \u0026lt;- data1[folds != i, ] #CREATE TRAINING SET test \u0026lt;- data1[folds == i, ] #CREATE TESTING SET truth \u0026lt;- test$y fit \u0026lt;- glm(y ~ region+new.MDR+pop.number+TB.100k+GDP, data = LassoTB, family = \u0026quot;binomial\u0026quot;) probs \u0026lt;- predict(fit, newdata = test, type = \u0026quot;response\u0026quot;) diags \u0026lt;- rbind(diags, class_diag(probs, truth)) } apply(diags, 2, mean) #AVERAGE THE DIAGNOSTICS ACROSS THE 10 FOLDS ## acc sens spec ppv auc ## 0.7976285 0.6386111 0.9093773 0.8284127 0.8826206 This new model’s out of sample accuracy is 0.797 which is fair. It is slightly higher than the previous model’s out of sample accuracy which was 0.743. The AUC also slightly increased from 0.839 to 0.886 (both models are good predictors of if there are XDR TB cases in a country).\n## R version 3.6.1 (2019-07-05) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS Mojave 10.14.6 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] glmnet_3.0-2 Matrix_1.2-18 plotROC_2.2.1 lmtest_0.9-37 zoo_1.8-7 sandwich_2.5-1 ## [7] forcats_0.5.0 stringr_1.4.0 dplyr_0.8.5 purrr_0.3.3 readr_1.3.1 tidyr_1.0.2 ## [13] tibble_2.1.3 ggplot2_3.3.0 tidyverse_1.3.0 knitr_1.28 ## ## loaded via a namespace (and not attached): ## [1] Rcpp_1.0.4 lubridate_1.7.4 lattice_0.20-40 foreach_1.4.8 assertthat_0.2.1 ## [6] digest_0.6.25 R6_2.4.1 cellranger_1.1.0 plyr_1.8.6 backports_1.1.5 ## [11] reprex_0.3.0 evaluate_0.14 httr_1.4.1 blogdown_0.18 pillar_1.4.3 ## [16] rlang_0.4.5 readxl_1.3.1 rstudioapi_0.11 rmarkdown_2.1 labeling_0.3 ## [21] splines_3.6.1 munsell_0.5.0 broom_0.5.5 compiler_3.6.1 modelr_0.1.6 ## [26] xfun_0.12 pkgconfig_2.0.3 shape_1.4.4 mgcv_1.8-31 htmltools_0.4.0 ## [31] tidyselect_1.0.0 bookdown_0.18 codetools_0.2-16 fansi_0.4.1 crayon_1.3.4 ## [36] dbplyr_1.4.2 withr_2.1.2 grid_3.6.1 nlme_3.1-145 jsonlite_1.6.1 ## [41] gtable_0.3.0 lifecycle_0.2.0 DBI_1.1.0 magrittr_1.5 scales_1.1.0 ## [46] cli_2.0.2 stringi_1.4.6 farver_2.0.3 fs_1.3.2 xml2_1.2.5 ## [51] generics_0.0.2 vctrs_0.2.4 iterators_1.0.12 tools_3.6.1 glue_1.3.2 ## [56] hms_0.5.3 yaml_2.2.1 colorspace_1.4-1 rvest_0.3.5 haven_2.2.0 ## [1] \u0026quot;2020-07-24 14:55:18 CDT\u0026quot; ## sysname ## \u0026quot;Darwin\u0026quot; ## release ## \u0026quot;18.7.0\u0026quot; ## version ## \u0026quot;Darwin Kernel Version 18.7.0: Tue Aug 20 16:57:14 PDT 2019; root:xnu-4903.271.2~2/RELEASE_X86_64\u0026quot; ## nodename ## \u0026quot;Cara-Yijin-Zou.local\u0026quot; ## machine ## \u0026quot;x86_64\u0026quot; ## login ## \u0026quot;yijinzou\u0026quot; ## user ## \u0026quot;yijinzou\u0026quot; ## effective_user ## \u0026quot;yijinzou\u0026quot;    ","date":1574726400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1574726400,"objectID":"1c09c7c94b97fd580673e1fcf04a5ff4","permalink":"/project/modeling_testing_predicting/","publishdate":"2019-11-26T00:00:00Z","relpermalink":"/project/modeling_testing_predicting/","section":"project","summary":" Examines the relationship between tuberculosis cases, mortality, resistance, etc. compared to country's GDP, population, etc. Using this data a variety of statistical tests and models are formed for prediction.","tags":["Model","Test","Predict","MANOVA","ANOVA","Post-hoc","PERMANOVA","Regression","Bootstrap","ROC","AOC","Confusion Matrix","Cross Validation","LASSO","Statistics"],"title":"Modeling, Testing, and Predicting","type":"project"},{"authors":null,"categories":["Statistics"],"content":" Data Wrangling and Data Exploration Introduction The first two datasets used in this project are both from World Health Organization. They contains information about cases of Tuberculosis for different countries including the new cases, previous cases, drug resistant cases, total population, etc. The third dataset is from The World Bank Group which lists the GDP per capita of countries for different years in US dollars. This data is interesting because I am currently in a reserach stream at UT trying to identify potential drugs to treat tuberculosis.\nThe first two datasets include information about the resistance of Mycobacterium tuberculosis to drugs. Tuberculosis is linked with poverty which is why the GDP per capita was also considered. Expectations include that as the number of TB (tuberculosis) cases increase, the number of drug resistant multidrug resistant (MDR) and extensively drug resistant (XDR) TB cases will also increase. Furthermore, as GDP per capita increases, TB cases (and mortality) per 100,000 individuals should decrease since it is assumed that there is more access to healthcare and resources for treatment and prevention.\n Find data:  Tidying: join1 \u0026lt;- left_join(data1a, data1b, by = c(\u0026quot;country\u0026quot;, \u0026quot;year\u0026quot;)) head(join1) ## country region year new.pul.TB prev.treated.pul.TB prev.unk.pul.TB ## 1 Afghanistan EMR 2017 19354 2233 125 ## 2 Afghanistan EMR 2018 20485 1712 NA ## 3 Albania EUR 2017 195 15 0 ## 4 Albania EUR 2018 198 10 0 ## 5 Algeria AFR 2017 6278 419 0 ## 6 Algeria AFR 2018 6137 362 21 ## new.MDR prev.MDR MDR.tested XDR pop.number TB.100k TB.num TB_mort.100k ## 1 NA NA NA 5 36296113 189 69000 30.00 ## 2 NA 10 10 8 37171921 189 70000 29.00 ## 3 0 0 0 0 2884169 20 580 0.34 ## 4 1 1 NA NA 2882740 18 510 0.34 ## 5 11 28 39 4 41389189 70 29000 7.70 ## 6 2 5 7 4 42228408 69 29000 7.70 ## TB_mort.num ## 1 11000 ## 2 11000 ## 3 10 ## 4 10 ## 5 3200 ## 6 3300 First, a left join was used to join the first two tuberculosis data sets since dataset 1a included information for 2017 and 2018 while dataset 1b contains data for many years (2000-2018). A left join was used compared to a full_join or right_join because there would be many rows with NAs (rows with year 2000-2016).\ndata2pivot \u0026lt;- data2 %\u0026gt;% pivot_longer(cols = c(3:4), names_to = \u0026quot;year\u0026quot;, values_to = \u0026quot;GDP\u0026quot;) %\u0026gt;% separate(col = \u0026quot;year\u0026quot;, into = c(NA, \u0026quot;year\u0026quot;), sep = 1) %\u0026gt;% mutate(year = as.numeric(year)) head(data2pivot) ## # A tibble: 6 x 4 ## country Country.Code year GDP ## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Aruba ABW 2017 25630. ## 2 Aruba ABW 2018 NA ## 3 Afghanistan AFG 2017 556. ## 4 Afghanistan AFG 2018 521. ## 5 Angola AGO 2017 4096. ## 6 Angola AGO 2018 3432. Then, to make it easier to join the first two datasets with the third dataset, pivot longer was used on the GDP data (dataset 2). This is due to the fact that while year is a column name in dataset 1a and 1b, for dataset 2, the GDP has two columns: 2017 and 2018. Additionally, when imported, the header of the GDP data inserted an X before the year value. This was removed using the seperate function. Mutate was then used on ‘year’ because its type was character which is incompatible with year in the tuberculosis (1a and 1b) dataset which is a numeric type (cannot be used to join).\n Joining/Merging nrow(join1) ## [1] 432 nrow(data2pivot) ## [1] 528 # Joining of all three datasets and deleting rows with NAs join2 \u0026lt;- inner_join(join1, data2pivot, by = c(\u0026quot;country\u0026quot;, \u0026quot;year\u0026quot;)) nrow(join1) - nrow(join2) ## [1] 72 nrow(data2pivot) - nrow(join2) ## [1] 168 join2 \u0026lt;- join2 %\u0026gt;% na.omit() nrow(join2) ## [1] 222 # document the type of join that you do # (left/right/inner/full), including how many cases in each # dataset were dropped and why you chose this particular join Initially, the tuberculosis dataset had 432 cases while the GDP dataset had 528 cases. An inner join was chosen so that all countries remaining would contain information for tuberculosis and their GDP. This resulted in 360 observations remaining with 72 cases being dropped from the tuberculosis dataset and 168 cases dropped from the GDP dataset. Problems include that rows with NAs are more likely to be smaller and poorer countries with less documentation of the data which may skew the results.\n Wrangling: filter, select, arrange, group_by, mutate, summarize # Determining the quantile of GDP and population for each # country ntile \u0026lt;- join2 %\u0026gt;% mutate(ntileGDP = ntile(n = 5, x = GDP)) %\u0026gt;% mutate(ntilepop = ntile(n = 5, x = pop.number)) # Mean and standard deviation of numeric variables (excluding # year) join2 %\u0026gt;% select_if(is.numeric) %\u0026gt;% select(-year) %\u0026gt;% summarize_all(.funs = mean) ## new.pul.TB prev.treated.pul.TB prev.unk.pul.TB new.MDR prev.MDR MDR.tested ## 1 16571.29 2829.351 138.8468 135.6441 197.5495 290.7928 ## XDR pop.number TB.100k TB.num TB_mort.100k TB_mort.num GDP ## 1 38.74324 33335846 98.5732 56241.17 16.46901 8344.464 14741.78 join2 %\u0026gt;% select_if(is.numeric) %\u0026gt;% select(-year) %\u0026gt;% summarize_all(.funs = sd) ## new.pul.TB prev.treated.pul.TB prev.unk.pul.TB new.MDR prev.MDR MDR.tested ## 1 84215.1 18464.84 1363.483 667.0942 1133.7 1601.977 ## XDR pop.number TB.100k TB.num TB_mort.100k TB_mort.num GDP ## 1 261.0853 131603854 135.356 276737.8 32.3854 44225.92 18664.98 join2 %\u0026gt;% summarize_all(.funs = n_distinct) ## country region year new.pul.TB prev.treated.pul.TB prev.unk.pul.TB new.MDR ## 1 130 6 2 205 148 44 77 ## prev.MDR MDR.tested XDR pop.number TB.100k TB.num TB_mort.100k TB_mort.num ## 1 77 86 43 222 152 142 139 132 ## Country.Code GDP ## 1 130 222 # top 10 Observations for extensively drug resistant TB cases # for 2017 and 2018 ntile %\u0026gt;% arrange(desc(XDR)) %\u0026gt;% head(10) ## country region year new.pul.TB prev.treated.pul.TB ## 1 Russian Federation EUR 2017 32978 26058 ## 2 Ukraine EUR 2017 12840 7212 ## 3 Ukraine EUR 2018 12931 6774 ## 4 India SEA 2018 825939 208197 ## 5 India SEA 2017 868769 176450 ## 6 Belarus EUR 2017 1690 700 ## 7 Tajikistan EUR 2017 2432 652 ## 8 Belarus EUR 2018 1529 612 ## 9 Pakistan EMR 2017 128806 15241 ## 10 Peru AMR 2018 17387 3075 ## prev.unk.pul.TB new.MDR prev.MDR MDR.tested XDR pop.number TB.100k TB.num ## 1 0 8206 14611 20477 3562 145530082 59 85000 ## 2 0 2594 2414 5008 1001 44487709 84 37000 ## 3 0 2755 2299 5054 972 44246156 80 36000 ## 4 0 3232 5182 6832 493 1352642280 199 2690000 ## 5 0 2152 5357 6787 466 1338676785 204 2740000 ## 6 0 629 459 1088 343 9450231 37 3500 ## 7 0 413 133 508 279 8880268 85 7500 ## 8 0 559 425 984 185 9452617 31 2900 ## 9 116 535 2102 2600 123 207906209 267 554000 ## 10 0 1198 481 758 91 31989260 123 39000 ## TB_mort.100k TB_mort.num Country.Code GDP ntileGDP ntilepop ## 1 8.1 12000 RUS 10750.5871 4 5 ## 2 14.0 6400 UKR 2640.6757 2 5 ## 3 13.0 5700 UKR 3095.1736 2 5 ## 4 33.0 449000 IND 2015.5905 2 5 ## 5 34.0 454000 IND 1981.4990 2 5 ## 6 6.0 560 BLR 5761.7471 3 3 ## 7 9.2 820 TJK 806.0416 1 3 ## 8 5.9 560 BLR 6289.9386 3 3 ## 9 21.0 45000 PAK 1466.8431 1 5 ## 10 8.3 2700 PER 6947.2566 3 5 # new variable created using mutate; proportion of XDR/MDR: # richest countries as well as the poorest countries have the # lowest mean percentage of MDR TB cases developing into XDR # cases ntile %\u0026gt;% mutate(perc.XDR.MDR = XDR/MDR.tested) %\u0026gt;% group_by(ntileGDP) %\u0026gt;% summarize(mean(perc.XDR.MDR, na.rm = T)) ## # A tibble: 5 x 2 ## ntileGDP `mean(perc.XDR.MDR, na.rm = T)` ## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 0.0709 ## 2 2 0.169 ## 3 3 0.150 ## 4 4 0.129 ## 5 5 0.0803 # Group by quantile of GDP per capita ; mean TB per 100,000 # people and mortality due to TB per 100,00 people appears to # decrease as the percentile of GDP per capita increases ntile %\u0026gt;% group_by(ntileGDP) %\u0026gt;% summarize(mean(TB.100k), mean(TB_mort.100k)) ## # A tibble: 5 x 3 ## ntileGDP `mean(TB.100k)` `mean(TB_mort.100k)` ## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 220 49.0 ## 2 2 163 19.4 ## 3 3 64.0 9.93 ## 4 4 29.0 2.61 ## 5 5 14.9 0.811 # Group by region in world: AFR=Africa; AMR=Americas; # EMR=Eastern Mediterranean; EUR=Europe; SEAR=South-East # Asia; WPR=Western Pacific join2 %\u0026gt;% group_by(region) %\u0026gt;% select_if(is.numeric) %\u0026gt;% select(-year) %\u0026gt;% summarize(mean(TB.100k), mean(TB_mort.100k)) ## # A tibble: 6 x 3 ## region `mean(TB.100k)` `mean(TB_mort.100k)` ## \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 AFR 210 53.0 ## 2 AMR 30.7 3.79 ## 3 EMR 76.6 10.2 ## 4 EUR 22.8 2.09 ## 5 SEA 198. 26.4 ## 6 WPR 153. 11.5 # Maximum TB cases per 100,000 people ntile %\u0026gt;% filter(TB.100k == max(TB.100k)) ## country region year new.pul.TB prev.treated.pul.TB prev.unk.pul.TB new.MDR ## 1 Lesotho AFR 2018 3595 728 0 51 ## prev.MDR MDR.tested XDR pop.number TB.100k TB.num TB_mort.100k TB_mort.num ## 1 5 5 0 2108328 611 13000 200 4200 ## Country.Code GDP ntileGDP ntilepop ## 1 LSO 1324.283 1 2 # Minimum TB cases per 100,000 people ntile %\u0026gt;% filter(TB.100k == min(TB.100k)) ## country region year new.pul.TB prev.treated.pul.TB prev.unk.pul.TB new.MDR ## 1 Barbados AMR 2017 0 0 0 0 ## 2 San Marino EUR 2017 0 0 0 0 ## prev.MDR MDR.tested XDR pop.number TB.100k TB.num TB_mort.100k TB_mort.num ## 1 0 0 0 286232 0 0 0.9 3 ## 2 0 0 0 33671 0 0 0.0 0 ## Country.Code GDP ntileGDP ntilepop ## 1 BRB 16327.61 4 1 ## 2 SMR 48494.55 5 1 # Group by two variables: percentile of population and year ntile %\u0026gt;% group_by(ntilepop, year) %\u0026gt;% summarize(mean(TB.100k), mean(TB_mort.100k)) ## # A tibble: 10 x 4 ## # Groups: ntilepop [5] ## ntilepop year `mean(TB.100k)` `mean(TB_mort.100k)` ## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 2017 89.1 9.21 ## 2 1 2018 91.8 10.4 ## 3 2 2017 117. 24.1 ## 4 2 2018 154. 31.6 ## 5 3 2017 23.3 2.31 ## 6 3 2018 20.9 1.93 ## 7 4 2017 106. 22.6 ## 8 4 2018 147. 33.8 ## 9 5 2017 124. 15.1 ## 10 5 2018 124. 16.4 Since most of the variables are numeric, the quantiles of GDP and the population were found and were used later to group the data. The mean and standard deviation were found for the numeric variables using the summarize_all function. Using arrange, the country that has the most extensively drug resistant TB cases for either 2017 or 2018 is the Russia followed by Ukraine and India. Using mutate to create a new variable, the proportion of of XDR/MDR appears to decrease towards the extremes of GDP per capita. Grouping by quantile of GDP per capita, mean TB cases and mortality due to TB per 100,00 people appears to decrease as the quantile of GDP per capita increases. Grouping by region, AFR has the highest mean TB cases and mortality per 100,000 people of the regions while EUR has the lowest averages.\nThe country with the maximum TB per 100,000 people is Lesotho in AFR and in the 1st quantile of GDP (low). The countries with the minimum TB per 100,000 people are Barbados and San Marino which both have small populations (1st quantile) and relatively high GDP. Grouping by two variables (population and year), countries in the 2nd and 4th quantile of population saw the greatest change in mean TB cases per 100,000 with both reporting an increase from 2017 to 2018. Countries in the 2nd and 4th quantile of the population also had the greatest mean mortality due to TB cases per 100,000 (as well as the greatest change between 2017 and 2018). Interestingly, the countries in the 3rd quantile had the lowest mean TB cases and mortality per 100,000 (for both 2017 and 2018).\n# Correlation matrix join2 %\u0026gt;% select_if(is.numeric) %\u0026gt;% cor() %\u0026gt;% round(2) ## year new.pul.TB prev.treated.pul.TB prev.unk.pul.TB ## year 1.00 0.00 0.01 0.06 ## new.pul.TB 0.00 1.00 0.97 0.17 ## prev.treated.pul.TB 0.01 0.97 1.00 0.06 ## prev.unk.pul.TB 0.06 0.17 0.06 1.00 ## new.MDR -0.03 0.39 0.47 0.02 ## prev.MDR -0.06 0.47 0.53 0.08 ## MDR.tested -0.05 0.43 0.49 0.07 ## XDR -0.06 0.18 0.26 0.01 ## pop.number 0.00 0.99 0.98 0.14 ## TB.100k 0.04 0.19 0.13 0.11 ## TB.num 0.00 0.99 0.96 0.21 ## TB_mort.100k 0.05 0.11 0.08 0.04 ## TB_mort.num 0.01 0.99 0.97 0.15 ## GDP 0.01 -0.13 -0.10 -0.02 ## new.MDR prev.MDR MDR.tested XDR pop.number TB.100k TB.num ## year -0.03 -0.06 -0.05 -0.06 0.00 0.04 0.00 ## new.pul.TB 0.39 0.47 0.43 0.18 0.99 0.19 0.99 ## prev.treated.pul.TB 0.47 0.53 0.49 0.26 0.98 0.13 0.96 ## prev.unk.pul.TB 0.02 0.08 0.07 0.01 0.14 0.11 0.21 ## new.MDR 1.00 0.96 0.98 0.96 0.43 0.04 0.38 ## prev.MDR 0.96 1.00 0.99 0.93 0.51 0.06 0.47 ## MDR.tested 0.98 0.99 1.00 0.96 0.47 0.05 0.42 ## XDR 0.96 0.93 0.96 1.00 0.23 0.00 0.17 ## pop.number 0.43 0.51 0.47 0.23 1.00 0.13 0.98 ## TB.100k 0.04 0.06 0.05 0.00 0.13 1.00 0.23 ## TB.num 0.38 0.47 0.42 0.17 0.98 0.23 1.00 ## TB_mort.100k 0.01 0.03 0.02 -0.01 0.07 0.86 0.13 ## TB_mort.num 0.38 0.46 0.42 0.17 0.98 0.21 0.98 ## GDP -0.09 -0.08 -0.09 -0.06 -0.10 -0.40 -0.13 ## TB_mort.100k TB_mort.num GDP ## year 0.05 0.01 0.01 ## new.pul.TB 0.11 0.99 -0.13 ## prev.treated.pul.TB 0.08 0.97 -0.10 ## prev.unk.pul.TB 0.04 0.15 -0.02 ## new.MDR 0.01 0.38 -0.09 ## prev.MDR 0.03 0.46 -0.08 ## MDR.tested 0.02 0.42 -0.09 ## XDR -0.01 0.17 -0.06 ## pop.number 0.07 0.98 -0.10 ## TB.100k 0.86 0.21 -0.40 ## TB.num 0.13 0.98 -0.13 ## TB_mort.100k 1.00 0.16 -0.32 ## TB_mort.num 0.16 1.00 -0.13 ## GDP -0.32 -0.13 1.00 The correlation matrix shows that the strongest positive correlations are between new pulmonary TB cases and population number; new pulmonary TB cases and total number of TB cases; new pulmonary TB cases and total mortality due to TB; previous MDR TB cases and number of MDR cases that were tested for additional resistance. The strongest negative correlations are GDP and the TB cases per 100,000 people; GDP and TB mortality per 100,000 people.\n Visualizing # GGPlot 1: Scatterplot ntile %\u0026gt;% ggplot(aes(x = TB.100k, y = TB_mort.100k)) + geom_point(aes(color = ntileGDP)) + ggtitle(\u0026quot;TB Cases and Mortality\u0026quot;) + xlab(\u0026quot;TB Cases per 100,000 people\u0026quot;) + ylab(\u0026quot;TB mortality per 100,000 people\u0026quot;) + scale_fill_brewer() + scale_y_continuous(breaks = c(25, 50, 75, 100, 125, 150, 175, 200)) + scale_x_continuous(breaks = c(100, 200, 300, 400, 500, 600)) + labs(color = \u0026quot;GDP\u0026quot;) + theme_classic() This graph shows that as TB cases increase, TB mortality also increases per 100,000 people. Contrastingly, TB cases and mortality have a negative relationship with GDP per capita. This suggests that countries with lower GDP have higher occurrences of TB and mortality due to TB which is expected since there is less funding towards preventative measures as well as resources and access to treatment.\n# GGPlot 2: Boxplot ntile %\u0026gt;% ggplot(aes(group = region, x = region, y = TB.100k)) + geom_boxplot() + geom_jitter(alpha = 0.3, aes(color = ntilepop, size = ntilepop)) + ggtitle(\u0026quot;TB cases for each Region\u0026quot;) + xlab(\u0026quot;Region\u0026quot;) + ylab(\u0026quot;TB Cases per 100,000 people\u0026quot;) + labs(color = \u0026quot;Population\u0026quot;, size = \u0026quot;Population\u0026quot;) + theme_light() The boxplot shows that the median of TB cases for 100,000 people is highest in SEA. Previously, summary statistics showed that AFR had the highest mean. From the boxplot, it is clear that the distribution is skewed resulting in a higher median than mean. The spread is largest for AFR as well. The countries in AMR, EMR, and EUR all have relatively low median values. When grouped by region, there is not an obvious relationship between the population and TB cases per 100,000 people.\n# GGPlot 3: Stacked Bar plot ntile$year \u0026lt;- factor(ntile$year, levels = c(\u0026quot;2017\u0026quot;, \u0026quot;2018\u0026quot;)) ggplot(ntile, aes(x = ntileGDP, y = TB_mort.100k, fill = year)) + geom_bar(stat = \u0026quot;summary\u0026quot;, fun.y = \u0026quot;mean\u0026quot;, position = \u0026quot;dodge\u0026quot;) + geom_errorbar(stat = \u0026quot;summary\u0026quot;, position = \u0026quot;dodge\u0026quot;) + xlab(\u0026quot;Quantile of GDP per capita\u0026quot;) + ylab(\u0026quot;TB Mortality per 100,000 people\u0026quot;) + ggtitle(\u0026quot;GDP per Capita and TB mortality per 100,000 people\u0026quot;) + theme_grey() + scale_y_continuous(breaks = c(10, 20, 30, 40, 50, 60)) A stacked bar plot was used to demonstrate how increasing GDP per capita results in a lower TB mortality on average. The biggest change in TB mortality between 2017 and 2018 is represented in the countries in the 1st quantile of GDP per capita (however, the error bars are overlapping). As the quantile of GDP increases, the spread of the data (error bars) also decrease.\n Dimensionality Reduction # Prepare data by scaling joinPCA \u0026lt;- join2 %\u0026gt;% select_if(is.numeric) %\u0026gt;% scale %\u0026gt;% na.omit join_pca \u0026lt;- princomp(joinPCA) summary(join_pca, loadings = T) ## Importance of components: ## Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 ## Standard deviation 2.5078967 1.6468586 1.4044384 1.01775753 0.96840866 ## Proportion of Variance 0.4512861 0.1946011 0.1415266 0.07432267 0.06728992 ## Cumulative Proportion 0.4512861 0.6458872 0.7874138 0.86173646 0.92902637 ## Comp.6 Comp.7 Comp.8 Comp.9 ## Standard deviation 0.86661846 0.371251458 0.202421301 0.145441708 ## Proportion of Variance 0.05388756 0.009889379 0.002939985 0.001517786 ## Cumulative Proportion 0.98291394 0.992803315 0.995743300 0.997261086 ## Comp.10 Comp.11 Comp.12 Comp.13 ## Standard deviation 0.127383707 0.1098210941 0.0756806126 0.052799805 ## Proportion of Variance 0.001164288 0.0008653747 0.0004109623 0.000200031 ## Cumulative Proportion 0.998425374 0.9992907485 0.9997017107 0.999901742 ## Comp.14 ## Standard deviation 3.700566e-02 ## Proportion of Variance 9.825826e-05 ## Cumulative Proportion 1.000000e+00 ## ## Loadings: ## Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6 Comp.7 Comp.8 ## year 0.729 0.674 ## new.pul.TB -0.356 0.246 -0.118 0.147 ## prev.treated.pul.TB -0.362 0.188 -0.138 -0.465 ## prev.unk.pul.TB 0.656 -0.728 0.101 ## new.MDR -0.297 -0.385 -0.616 ## prev.MDR -0.320 -0.344 0.524 ## MDR.tested -0.310 -0.372 0.177 ## XDR -0.235 -0.474 0.124 ## pop.number -0.361 0.210 -0.147 ## TB.100k 0.213 0.602 -0.241 0.708 ## TB.num -0.354 0.253 -0.100 0.131 0.239 ## TB_mort.100k 0.192 0.602 -0.358 -0.670 ## TB_mort.num -0.354 0.255 -0.101 -0.115 ## GDP -0.404 0.157 -0.890 ## Comp.9 Comp.10 Comp.11 Comp.12 Comp.13 Comp.14 ## year ## new.pul.TB -0.304 0.175 -0.790 -0.156 ## prev.treated.pul.TB 0.688 -0.130 0.264 -0.111 ## prev.unk.pul.TB ## new.MDR -0.402 -0.197 -0.295 -0.212 0.203 ## prev.MDR 0.382 -0.267 -0.185 -0.219 -0.151 0.416 ## MDR.tested -0.102 0.177 -0.821 ## XDR 0.537 0.512 0.332 0.201 ## pop.number -0.139 -0.605 0.616 0.164 ## TB.100k ## TB.num -0.307 0.115 -0.437 0.509 0.358 0.199 ## TB_mort.100k ## TB_mort.num 0.424 -0.667 0.390 ## GDP # Choose number of PC to keep convert standard deviations to # eigenvalues eigval \u0026lt;- join_pca$sdev^2 # proportion of variance explained by each PC varprop = round(eigval/sum(eigval), 2) ggplot() + geom_bar(aes(y = varprop, x = 1:14), stat = \u0026quot;identity\u0026quot;) + xlab(\u0026quot;\u0026quot;) + geom_path(aes(y = varprop, x = 1:14)) + geom_text(aes(x = 1:14, y = varprop, label = round(varprop, 2)), vjust = 1, col = \u0026quot;white\u0026quot;, size = 5) + scale_y_continuous(breaks = seq(0, 0.6, 0.2), labels = scales::percent) + scale_x_continuous(breaks = 1:14) # Plot for PCA (PC1 and PC2) join2 %\u0026gt;% na.omit %\u0026gt;% mutate(PC1 = join_pca$scores[, 1], PC2 = join_pca$scores[, 2]) %\u0026gt;% ggplot(aes(x = PC1, y = PC2, color = region, size = GDP)) + geom_point() + ggtitle(\u0026quot;PCA Plot\u0026quot;) + theme(legend.position = \u0026quot;none\u0026quot;) # Plot for PCA (PC3 and PC4) join2 %\u0026gt;% na.omit %\u0026gt;% mutate(PC3 = join_pca$scores[, 3], PC4 = join_pca$scores[, 4]) %\u0026gt;% ggplot(aes(x = PC3, y = PC4, color = region, size = GDP)) + geom_point() + ggtitle(\u0026quot;PCA Plot\u0026quot;) + theme(legend.position = \u0026quot;none\u0026quot;) # Plot of loadings join_pca$loadings[1:14, 1:2] %\u0026gt;% as.data.frame %\u0026gt;% rownames_to_column %\u0026gt;% ggplot() + geom_hline(aes(yintercept = 0), lty = 2) + geom_vline(aes(xintercept = 0), lty = 2) + ylab(\u0026quot;PC2\u0026quot;) + xlab(\u0026quot;PC1\u0026quot;) + geom_segment(aes(x = 0, y = 0, xend = Comp.1, yend = Comp.2), arrow = arrow(), col = \u0026quot;red\u0026quot;) + geom_label(aes(x = Comp.1 * 1.1, y = Comp.2 * 1.1, label = rowname)) + ggtitle(\u0026quot;Plot of Loadings\u0026quot;) # biplot combining loadings plot and PC score plot library(\u0026quot;factoextra\u0026quot;) fviz_pca_biplot(join_pca) Based on the scree plot and cumulative proportion of variance (and Kaiser’s rule), 3 to 4 PCs should be chosen. High scores on PC1 indicate low TB cases (new, previous, MDR TB, mortality, etc.) and low population. For PC2, high scores indicate high TB cases and mortality per 100,000 people but lower resistant (MDR and XDR) TB cases as well as lower GDP per capita. For PC3, high scores indicate lower cases of resistant TB cases and lower TB cases and mortality per 100,000 people but higher GDP. On the PCA plot, there appears to be some seperation based on GDP looking at PC3 (larger dots on the right). Finally, high scores PC4 indicate high numbers of confirmed TB cases with unknown TB treatment history.\nThe plot of loadings helps visualize which variances contribute to which of the PCs with a smaller angle between vectors showing higher correlation. Therefore, GDP is negatively correlated to the other variables and mainly differs based on PC1. Contrastingly, other variables such as new cases of pulmonary TB, previous treated pulmonary TB cases, total TB cases, etc. are almost redundant.\n## R version 3.6.1 (2019-07-05) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS Mojave 10.14.6 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] factoextra_1.0.6 forcats_0.5.0 stringr_1.4.0 dplyr_0.8.5 ## [5] purrr_0.3.3 readr_1.3.1 tidyr_1.0.2 tibble_2.1.3 ## [9] ggplot2_3.3.0 tidyverse_1.3.0 knitr_1.28 ## ## loaded via a namespace (and not attached): ## [1] tidyselect_1.0.0 xfun_0.12 haven_2.2.0 lattice_0.20-40 ## [5] colorspace_1.4-1 vctrs_0.2.4 generics_0.0.2 htmltools_0.4.0 ## [9] yaml_2.2.1 utf8_1.1.4 rlang_0.4.5 ggpubr_0.2.5 ## [13] pillar_1.4.3 withr_2.1.2 glue_1.3.2 DBI_1.1.0 ## [17] dbplyr_1.4.2 modelr_0.1.6 readxl_1.3.1 lifecycle_0.2.0 ## [21] ggsignif_0.6.0 munsell_0.5.0 blogdown_0.18 gtable_0.3.0 ## [25] cellranger_1.1.0 rvest_0.3.5 codetools_0.2-16 evaluate_0.14 ## [29] labeling_0.3 fansi_0.4.1 broom_0.5.5 Rcpp_1.0.4 ## [33] formatR_1.7 backports_1.1.5 scales_1.1.0 jsonlite_1.6.1 ## [37] farver_2.0.3 fs_1.3.2 hms_0.5.3 digest_0.6.25 ## [41] stringi_1.4.6 ggrepel_0.8.2 bookdown_0.18 grid_3.6.1 ## [45] cli_2.0.2 tools_3.6.1 magrittr_1.5 crayon_1.3.4 ## [49] pkgconfig_2.0.3 ellipsis_0.3.0 xml2_1.2.5 reprex_0.3.0 ## [53] lubridate_1.7.4 assertthat_0.2.1 rmarkdown_2.1 httr_1.4.1 ## [57] rstudioapi_0.11 R6_2.4.1 nlme_3.1-145 compiler_3.6.1 ## [1] \u0026quot;2020-07-24 14:53:51 CDT\u0026quot; ## sysname ## \u0026quot;Darwin\u0026quot; ## release ## \u0026quot;18.7.0\u0026quot; ## version ## \u0026quot;Darwin Kernel Version 18.7.0: Tue Aug 20 16:57:14 PDT 2019; root:xnu-4903.271.2~2/RELEASE_X86_64\u0026quot; ## nodename ## \u0026quot;Cara-Yijin-Zou.local\u0026quot; ## machine ## \u0026quot;x86_64\u0026quot; ## login ## \u0026quot;yijinzou\u0026quot; ## user ## \u0026quot;yijinzou\u0026quot; ## effective_user ## \u0026quot;yijinzou\u0026quot;   ","date":1571529600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571529600,"objectID":"3fde0c916d62ac3081823b9e833541ca","permalink":"/project/exploratory_data_analysis/","publishdate":"2019-10-20T00:00:00Z","relpermalink":"/project/exploratory_data_analysis/","section":"project","summary":" Visualization of the relationship between tuberculosis cases, mortality, resistance, etc. compared to country's GDP, population, etc. Tidyverse is used for data cleaning while ggplot2 was used for graphing.","tags":["Tidyverse","ggplot2","Statistics","R"],"title":"Exploratory Data Analysis","type":"project"},{"authors":["Cara (Yijin) Zou"],"categories":null,"content":" Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1554595200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554595200,"objectID":"557dc08fd4b672a0c08e0a8cf0c9ff7d","permalink":"/publication/preprint/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/preprint/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example preprint / working paper","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Academic  Academic | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click  PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions?  Ask\n Documentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Cara (Yijin) Zou","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441065600,"objectID":"966884cc0d8ac9e31fab966c4534e973","permalink":"/publication/journal-article/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/journal-article/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example journal article","type":"publication"},{"authors":["Cara (Yijin) Zou","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372636800,"objectID":"69425fb10d4db090cfbd46854715582c","permalink":"/publication/conference-paper/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/conference-paper/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example conference paper","type":"publication"}]